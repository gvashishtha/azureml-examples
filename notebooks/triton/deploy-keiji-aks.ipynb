{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a question-answering model to the Triton Inference Server on NVIDIA Tesla V100s in Azure Kubernetes Service\n",
    "\n",
    "This notebook shows you how to deploy a Bi-Directional Attention Flow question-ansewring model to the high-performance [Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html) on Azure Kubernetes Service (AKS) graphical processing units (GPUs).\n",
    "\n",
    "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview steps\n",
    "\n",
    "Necessary only while this feature is in preview, will be unnecessary in a future release of the Azure Machine Learning Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "Model.Framework.MULTI = \"Multi\"\n",
    "Model._SUPPORTED_FRAMEWORKS_FOR_NO_CODE_DEPLOY.append(Model.Framework.MULTI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully downloaded model: densenet_onnx\n",
      "successfully downloaded model: bidaf-9\n",
      "successfully downloaded model: keiji_model\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# get the root of the repo\n",
    "prefix = Path(git.Repo(\".\", search_parent_directories=True).working_tree_dir)\n",
    "\n",
    "# Enables us to import helper functions as Python modules\n",
    "path_to_insert = prefix.joinpath(\"code\", \"deployment\", \"triton\").__str__()\n",
    "if path_to_insert not in sys.path:\n",
    "    sys.path.insert(1, path_to_insert)\n",
    "\n",
    "from model_utils import download_triton_models\n",
    "\n",
    "download_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model bidaf-9-example\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples'), name=bidaf-9-example, id=bidaf-9-example:29, version=29, tags={'area': 'Keiji model'}, properties={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_path = prefix.joinpath(\"models\", \"triton\")\n",
    "\n",
    "model = Model.register(\n",
    "    model_path=model_path,\n",
    "    model_name=\"bidaf-9-example\",\n",
    "    tags={\"area\": \"Keiji model\"},\n",
    "    description=\"Question answering from ONNX model zoo\",\n",
    "    workspace=ws,\n",
    "    model_framework=Model.Framework.MULTI,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy webservice\n",
    "\n",
    "Deploy to a pre-created [AksCompute](https://docs.microsoft.com/python/api/azureml-core/azureml.core.compute.aks.akscompute?view=azure-ml-py#provisioning-configuration-agent-count-none--vm-size-none--ssl-cname-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--location-none--vnet-resourcegroup-name-none--vnet-name-none--subnet-name-none--service-cidr-none--dns-service-ip-none--docker-bridge-cidr-none--cluster-purpose-none--load-balancer-type-none-) named `aks-gpu-deploy`. For other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running.............\n",
      "Succeeded\n",
      "AKS service creation operation finished, operation \"Succeeded\"\n",
      "CPU times: user 176 ms, sys: 55.8 ms, total: 231 ms\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from azureml.core.webservice import AksWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from random import randint\n",
    "\n",
    "service_name = \"triton-keiji-test\" + str(randint(10000, 99999))\n",
    "\n",
    "config = AksWebservice.deploy_configuration(\n",
    "    compute_target_name=\"aks-gpu-deploy\",\n",
    "    gpu_cores=1,\n",
    "    cpu_cores=1,\n",
    "    memory_gb=4,\n",
    "#     autoscale_min_replicas=3,\n",
    "#     autoscale_max_replics=8,\n",
    "    auth_enabled=False,\n",
    ")\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[model],\n",
    "    deployment_config=config,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tritonhttpclient.InferenceServerClient at 0x7f31caff11d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import triton_init, get_model_info\n",
    "\n",
    "triton_init(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model: bidaf-9, version: 1,               input meta: [{'name': 'query_char', 'datatype': 'BYTES', 'shape': [-1, 1, 1, 16]}, {'name': 'query_word', 'datatype': 'BYTES', 'shape': [-1, 1]}, {'name': 'context_word', 'datatype': 'BYTES', 'shape': [-1, 1]}, {'name': 'context_char', 'datatype': 'BYTES', 'shape': [-1, 1, 1, 16]}], input config: [{'name': 'query_char', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1', '1', '16']}, {'name': 'query_word', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1']}, {'name': 'context_word', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1']}, {'name': 'context_char', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1', '1', '16']}],               output_meta: [{'name': 'end_pos', 'datatype': 'INT32', 'shape': [1]}, {'name': 'start_pos', 'datatype': 'INT32', 'shape': [1]}], output config: [{'name': 'end_pos', 'data_type': 'TYPE_INT32', 'dims': ['1']}, {'name': 'start_pos', 'data_type': 'TYPE_INT32', 'dims': ['1']}]\n",
      "Found model: densenet_onnx, version: 1,               input meta: [{'name': 'data_0', 'datatype': 'FP32', 'shape': [3, 224, 224]}], input config: [{'name': 'data_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NCHW', 'dims': ['3', '224', '224'], 'reshape': {'shape': ['1', '3', '224', '224']}}],               output_meta: [{'name': 'fc6_1', 'datatype': 'FP32', 'shape': [1000]}], output config: [{'name': 'fc6_1', 'data_type': 'TYPE_FP32', 'dims': ['1000'], 'label_filename': 'densenet_labels.txt', 'reshape': {'shape': ['1', '1000', '1', '1']}}]\n",
      "Found model: keiji_model, version: 1,               input meta: [{'name': 'msg_type_ids', 'datatype': 'INT64', 'shape': [-1, 64]}, {'name': 'msg_ids', 'datatype': 'INT64', 'shape': [-1, 64]}, {'name': 'msg_mask', 'datatype': 'INT64', 'shape': [-1, 64]}], input config: [{'name': 'msg_type_ids', 'data_type': 'TYPE_INT64', 'dims': ['-1', '64']}, {'name': 'msg_ids', 'data_type': 'TYPE_INT64', 'dims': ['-1', '64']}, {'name': 'msg_mask', 'data_type': 'TYPE_INT64', 'dims': ['-1', '64']}],               output_meta: [{'name': 'out_0', 'datatype': 'INT64', 'shape': [3]}], output config: [{'name': 'out_0', 'data_type': 'TYPE_INT64', 'dims': ['3']}]\n"
     ]
    }
   ],
   "source": [
    "get_model_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE to KEIJI:\n",
    "# your model has been deployed successfully, now you just need to call into it\n",
    "# Check out the run() method in bidaf_utils for inspiration. You'll have to use \n",
    "# your own tokenizer (I don't know what this model expects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the webservice and the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model_utils import delete_triton_models\n",
    "\n",
    "service.delete()\n",
    "delete_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Try reading [our documentation](https://aka.ms/triton-aml-docs) to use Triton with your own models or check out the other notebooks in this folder for ways to do pre- and post-processing on the server. "
   ]
  }
 ],
 "metadata": {
  "index": {
   "compute": "AKS - GPU",
   "frameworks": "onnx",
   "other": "triton no-code deploy",
   "scenario": "deployment"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "deploy-bidaf-aks",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
