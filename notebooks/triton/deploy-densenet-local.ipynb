{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy an image classification model to the Triton Inference Server on local compute\n",
    "\n",
    "This notebook shows you how to deploy a Densenet image classification model to the high-performance [Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html) on local compute.\n",
    "\n",
    "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully downloaded model: densenet_onnx\n",
      "successfully downloaded model: bidaf-9\n",
      "successfully downloaded model: keiji_model\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# get the root of the repo\n",
    "prefix = Path(git.Repo(\".\", search_parent_directories=True).working_tree_dir)\n",
    "\n",
    "# Enables us to import helper functions as Python modules\n",
    "path_to_insert = prefix.joinpath(\"code\", \"deployment\", \"triton\").__str__()\n",
    "if path_to_insert not in sys.path:\n",
    "    sys.path.insert(1, path_to_insert)\n",
    "\n",
    "from model_utils import download_triton_models, delete_triton_models\n",
    "\n",
    "\n",
    "download_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model densenet-onnx-example\n",
      "Model(workspace=Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples'), name=densenet-onnx-example, id=densenet-onnx-example:36, version=36, tags={'area': 'Image classification', 'type': 'classification'}, properties={})\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_path = prefix.joinpath(\"models\")\n",
    "\n",
    "model = Model.register(\n",
    "    model_path=model_path,\n",
    "    model_name=\"densenet-onnx-example\",\n",
    "    tags={\"area\": \"Image classification\", \"type\": \"classification\"},\n",
    "    description=\"Image classification trained on Imagenet Dataset\",\n",
    "    workspace=ws,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy webservice\n",
    "\n",
    "In this case we deploy to the local compute, but for other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model densenet-onnx-example:36 to /tmp/azureml_2mm7bl2w/densenet-onnx-example/36\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
      "Logging into Docker registry 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_84a593724077248bb83b9e6a297a2562\n",
      " ---> 3ba78c3888f6\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> f77d30488572\n",
      "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjY1NjA1NzVkLWZhMDYtNGU3ZC05NWZiLWY5NjJlNzRlZmQ3YSIsInJlc291cmNlR3JvdXBOYW1lIjoiYXp1cmVtbC1leGFtcGxlcyIsImFjY291bnROYW1lIjoiZGVmYXVsdCIsIndvcmtzcGFjZUlkIjoiMGUxNDk3NjQtMzcyMC00NjEwLWIwZjMtM2UzZjk3NDU0NGFjIn0sIm1vZGVscyI6e30sIm1vZGVsc0luZm8iOnt9fQ== | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> Running in fdd2c2324354\n",
      "\u001b[91m/bin/bash: /azureml-envs/azureml_8e0ba8a051f495f86bcb9636c023c52d/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n",
      "\u001b[0m ---> 9a80660966a7\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmpc4s_qk5_.py' /var/azureml-app/main.py\n",
      " ---> Running in c794924ae98a\n",
      "\u001b[91m/bin/bash: /azureml-envs/azureml_8e0ba8a051f495f86bcb9636c023c52d/lib/libtinfo.so.5: no version information available (required by /bin/bash)\n",
      "\u001b[0m ---> 6fece0ed3a47\n",
      "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
      " ---> Running in d9b03f991f34\n",
      " ---> 816685137952\n",
      "Successfully built 816685137952\n",
      "Successfully tagged triton-densenet-onnx-local94516:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Docker container start has failed, the port you are attempting to use is already in use:\n",
      "500 Server Error: Internal Server Error (\"driver failed programming external connectivity on endpoint wizardly_goldstine (35b4f19a4d742a3b81567def388091b4acb6612b674553048d79066d91a425a8): Bind for 127.0.0.1:6789 failed: port is already allocated\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Docker container...\n"
     ]
    },
    {
     "ename": "WebserviceException",
     "evalue": "WebserviceException:\n\tMessage: Docker container start has failed, the port you are attempting to use is already in use:\n500 Server Error: Internal Server Error (\"driver failed programming external connectivity on endpoint wizardly_goldstine (35b4f19a4d742a3b81567def388091b4acb6612b674553048d79066d91a425a8): Bind for 127.0.0.1:6789 failed: port is already allocated\")\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Docker container start has failed, the port you are attempting to use is already in use:\\n500 Server Error: Internal Server Error (\\\"driver failed programming external connectivity on endpoint wizardly_goldstine (35b4f19a4d742a3b81567def388091b4acb6612b674553048d79066d91a425a8): Bind for 127.0.0.1:6789 failed: port is already allocated\\\")\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/docker/api/client.py\u001b[0m in \u001b[0;36m_raise_for_status\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http+docker://localhost/v1.40/containers/b783b76f8b221c7cb7176df9b9ebd0f51ff66864011bbe3b280f1ebf7ea1d62e/start",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/_model_management/_util.py\u001b[0m in \u001b[0;36mstart_docker_container\u001b[0;34m(container)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/docker/models/containers.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \"\"\"\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/docker/utils/decorators.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, resource_id, *args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 )\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/docker/api/container.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, container, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/docker/api/client.py\u001b[0m in \u001b[0;36m_raise_for_status\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mcreate_api_error_from_http_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/docker/errors.py\u001b[0m in \u001b[0;36mcreate_api_error_from_http_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNotFound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplanation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplanation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAPIError\u001b[0m: 500 Server Error: Internal Server Error (\"driver failed programming external connectivity on endpoint wizardly_goldstine (35b4f19a4d742a3b81567def388091b4acb6612b674553048d79066d91a425a8): Bind for 127.0.0.1:6789 failed: port is already allocated\")",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5be35ffffc1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0minference_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mdeployment_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(workspace, name, models, inference_config, deployment_config, deployment_target, overwrite)\u001b[0m\n\u001b[1;32m   1584\u001b[0m             return deployment_config._webservice_type._deploy(workspace, name, models,\n\u001b[1;32m   1585\u001b[0m                                                               \u001b[0minference_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m                                                               deployment_config=deployment_config)\n\u001b[0m\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0;31m# IotWebservice does not support environment-style deployment,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(workspace, name, models, image_config, deployment_config, wait, inference_config)\u001b[0m\n\u001b[1;32m    730\u001b[0m                        \u001b[0minference_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                        \u001b[0mdeployment_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployment_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                        wait=wait)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n\u001b[1;32m     70\u001b[0m                                           logger=module_logger)\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, models, image_config, deployment_config, wait, inference_config)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_docker_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_deployment_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmust_exist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36m_run_container\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;31m# Engage!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m         \u001b[0mstart_docker_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# Record the state of the deployment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/_model_management/_util.py\u001b[0m in \u001b[0;36mstart_docker_container\u001b[0;34m(container)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'port is already allocated'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplanation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             raise WebserviceException('Docker container start has failed, the port you are attempting to use '\n\u001b[0;32m--> 493\u001b[0;31m                                       'is already in use:\\n{}'.format(e), logger=module_logger)\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mWebserviceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Docker container start has failed:\\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Docker container start has failed, the port you are attempting to use is already in use:\n500 Server Error: Internal Server Error (\"driver failed programming external connectivity on endpoint wizardly_goldstine (35b4f19a4d742a3b81567def388091b4acb6612b674553048d79066d91a425a8): Bind for 127.0.0.1:6789 failed: port is already allocated\")\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Docker container start has failed, the port you are attempting to use is already in use:\\n500 Server Error: Internal Server Error (\\\"driver failed programming external connectivity on endpoint wizardly_goldstine (35b4f19a4d742a3b81567def388091b4acb6612b674553048d79066d91a425a8): Bind for 127.0.0.1:6789 failed: port is already allocated\\\")\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "from random import randint\n",
    "\n",
    "service_name = \"triton-densenet-onnx-local\" + str(randint(10000, 99999))\n",
    "env = Environment.get(ws, \"AzureML-Triton\").clone(\"triton-example\")\n",
    "\n",
    "for pip_package in [\"pillow\"]:\n",
    "    env.python.conda_dependencies.add_pip_package(pip_package)\n",
    "\n",
    "inference_config = InferenceConfig(\n",
    "    # this entry script is where we dispatch a call to the Triton server\n",
    "    entry_script=\"score_densenet.py\",\n",
    "    source_directory=prefix.joinpath(\"code\", \"deployment\", \"triton\"),\n",
    "    environment=env,\n",
    ")\n",
    "\n",
    "config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[model],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=config,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Content-Type\": \"application/octet-stream\"}\n",
    "\n",
    "data_file = prefix.joinpath(\"data\", \"raw\", \"images\", \"peacock.jpg\")\n",
    "test_sample = open(data_file, \"rb\").read()\n",
    "resp = requests.post(service.scoring_uri, data=test_sample, headers=headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the webservice and the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service.delete()\n",
    "delete_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Try changing the deployment configuration to [deploy to Azure Kubernetes Service](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python) for higher availability and better scalability."
   ]
  }
 ],
 "metadata": {
  "index": {
   "compute": "Local",
   "frameworks": "onnx",
   "other": "triton",
   "scenario": "deployment"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "deploy-densenet-local",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
