{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy to Triton Inference Server locally\n",
        "\n",
        "description: (preview) deploy an image classification model trained on densenet locally via Triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "ws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download model\n",
        "\n",
        "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "successfully downloaded model: densenet_onnx\n",
            "successfully downloaded model: bidaf-9\n"
          ]
        }
      ],
      "source": [
        "import git\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# get the root of the repo\n",
        "prefix = Path(git.Repo(\".\", search_parent_directories=True).working_tree_dir)\n",
        "\n",
        "# Enables us to import helper functions as Python modules\n",
        "path_to_insert = prefix.joinpath(\"code\", \"deployment\", \"triton\").__str__()\n",
        "if path_to_insert not in sys.path:\n",
        "    sys.path.insert(1, path_to_insert)\n",
        "\n",
        "from model_utils import download_triton_models, delete_triton_models\n",
        "\n",
        "\n",
        "download_triton_models(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register model\n",
        "\n",
        "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering model densenet-onnx-example\n",
            "Model(workspace=Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples'), name=densenet-onnx-example, id=densenet-onnx-example:531, version=531, tags={'area': 'Image classification', 'type': 'classification'}, properties={})\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model_path = prefix.joinpath(\"models\")\n",
        "\n",
        "model = Model.register(\n",
        "    model_path=model_path,\n",
        "    model_name=\"densenet-onnx-example\",\n",
        "    tags={\"area\": \"Image classification\", \"type\": \"classification\"},\n",
        "    description=\"Image classification trained on Imagenet Dataset\",\n",
        "    workspace=ws,\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy webservice\n",
        "\n",
        "In this case we deploy to the local compute, but for other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model densenet-onnx-example:531 to /tmp/azureml_5wnq75zm/densenet-onnx-example/531\n",
            "Generating Docker build context.\n",
            "Package creation Succeeded\n",
            "Logging into Docker registry 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
            "Logging into Docker registry 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
            "Building Docker image from Dockerfile...\n",
            "Step 1/5 : FROM 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_4c2f3a03e46a59f88531c71f6ec2e688\n",
            " ---> 9488af85a207\n",
            "Step 2/5 : COPY azureml-app /var/azureml-app\n",
            " ---> 434e8e2f03da\n",
            "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjY1NjA1NzVkLWZhMDYtNGU3ZC05NWZiLWY5NjJlNzRlZmQ3YSIsInJlc291cmNlR3JvdXBOYW1lIjoiYXp1cmVtbC1leGFtcGxlcyIsImFjY291bnROYW1lIjoiZGVmYXVsdCIsIndvcmtzcGFjZUlkIjoiMGUxNDk3NjQtMzcyMC00NjEwLWIwZjMtM2UzZjk3NDU0NGFjIn0sIm1vZGVscyI6e30sIm1vZGVsc0luZm8iOnt9fQ== | base64 --decode > /var/azureml-app/model_config_map.json\n",
            " ---> Running in a47d1ae19f73\n",
            " ---> c8c2f3d0ac4e\n",
            "Step 4/5 : RUN mv '/var/azureml-app/tmplheb9quf.py' /var/azureml-app/main.py\n",
            " ---> Running in ca9aca344c5e\n",
            " ---> 8f57841dc70b\n",
            "Step 5/5 : CMD [\"bash\",\"-c\",\"source activate '/opt/miniconda'; export AZUREML_CONDA_ENVIRONMENT_PATH=${AZUREML_CONDA_ENVIRONMENT_PATH:=$CONDA_PREFIX}; exec 'runsvdir' '/var/runit'\"]\n",
            " ---> Running in 1cb817d9d215\n",
            " ---> 5b587d762ac5\n",
            "Successfully built 5b587d762ac5\n",
            "Successfully tagged triton-densenet-onnx-local70703:latest\n",
            "Starting Docker container...\n",
            "Docker container running.\n",
            "Checking container health...\n",
            "WARNING - The container is missing requirements for the Azure Machine Learning serving stack. Please make sure the \"azureml-defaults\" pip package is included in your environment.\n",
            "ERROR - Error: Container has crashed. Did your init method fail?\n",
            "\n",
            "\n",
            "Container Logs:\n",
            "2020-11-06T19:38:30,208456800+00:00 - iot-server/run \n",
            "2020-11-06T19:38:30,208483800+00:00 - gunicorn/run \n",
            "2020-11-06T19:38:30,209136300+00:00 - rsyslog/run \n",
            "2020-11-06T19:38:30,212484900+00:00 - triton/run \n",
            "2020-11-06T19:38:30,213213200+00:00 - nginx/run \n",
            "./run: line 14: exec: gunicorn: not found\n",
            "\n",
            "=============================\n",
            "== Triton Inference Server ==\n",
            "=============================\n",
            "\n",
            "NVIDIA Release 20.08 (build 15533555)\n",
            "\n",
            "Copyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n",
            "\n",
            "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
            "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
            "project or file.\n",
            "2020-11-06T19:38:30,266443500+00:00 - gunicorn/finish 127 0\n",
            "2020-11-06T19:38:30,268800800+00:00 - Exit code 127 is not normal. Killing image.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "WebserviceException",
          "evalue": "WebserviceException:\n\tMessage: Error: Container has crashed. Did your init method fail?\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Error: Container has crashed. Did your init method fail?\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-64546c8dea3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_deployment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n\u001b[1;32m     70\u001b[0m                                           logger=module_logger)\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mwait_for_deployment\u001b[0;34m(self, show_output)\u001b[0m\n\u001b[1;32m    601\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                                    \u001b[0mhealth_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_base_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                                    cleanup_if_failed=False)\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalWebservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATE_RUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/_model_management/_util.py\u001b[0m in \u001b[0;36mcontainer_health_check\u001b[0;34m(docker_port, container, health_url, cleanup_if_failed)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;31m# The container has started and crashed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             _raise_for_container_failure(container, cleanup_if_failed,\n\u001b[0;32m--> 741\u001b[0;31m                                          'Error: Container has crashed. Did your init method fail?')\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;31m# The container hasn't crashed, so try to ping the health endpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/_model_management/_util.py\u001b[0m in \u001b[0;36m_raise_for_container_failure\u001b[0;34m(container, cleanup, message)\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0mcleanup_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mWebserviceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Error: Container has crashed. Did your init method fail?\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Error: Container has crashed. Did your init method fail?\"\n    }\n}"
          ]
        }
      ],
      "source": [
        "from azureml.core.webservice import LocalWebservice\n",
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.model import InferenceConfig\n",
        "from random import randint\n",
        "\n",
        "service_name = \"triton-densenet-onnx-local\" + str(randint(10000, 99999))\\\n",
        "\n",
        "# This doesn't work because the install order is not respected\n",
        "# env = Environment.get(ws, \"AzureML-Triton\").clone(\"triton-example\")\n",
        "\n",
        "# for pip_package in [\"pillow\", \"nvidia-pyindex\", \"tritonclient[http]\"]:\n",
        "#     env.python.conda_dependencies.add_pip_package(pip_package)\n",
        "\n",
        "\n",
        "env = Environment(\"triton-example\")\n",
        "env.docker.base_image = None\n",
        "env.docker.base_dockerfile=prefix.joinpath(\"notebooks\", \"triton\", \"docker\", \"Dockerfile\")\n",
        "env.python.user_managed_dependencies=True\n",
        "env.python.interpreter_path='/opt/miniconda/bin/python'\n",
        "env.inferencing_stack_version='latest'\n",
        "\n",
        "\n",
        "# conda_dep.add_pip_package('nvidia-pyindex')\n",
        "# conda_dep.add_pip_package('tritonclient')\n",
        "# conda_dep.add_pip_package('pillow')\n",
        "#env.python.conda_dependencies = conda_dep\n",
        "env.environment_variables['WORKER_COUNT']='1'\n",
        "\n",
        "inference_config = InferenceConfig(\n",
        "    # this entry script is where we dispatch a call to the Triton server\n",
        "    entry_script=\"score_densenet.py\",\n",
        "    source_directory=prefix.joinpath(\"code\", \"deployment\", \"triton\"),\n",
        "    environment=env,\n",
        ")\n",
        "\n",
        "config = LocalWebservice.deploy_configuration(port=6789)\n",
        "\n",
        "service = Model.deploy(\n",
        "    workspace=ws,\n",
        "    name=service_name,\n",
        "    models=[model],\n",
        "    inference_config=inference_config,\n",
        "    deployment_config=config,\n",
        "    overwrite=True,\n",
        ")\n",
        "\n",
        "service.wait_for_deployment(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Container has been successfully cleaned up.\n",
            "Starting Docker container...\n",
            "Docker container running.\n"
          ]
        }
      ],
      "source": [
        "service.reload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the webservice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"message\": \"Expects Content-Type to be application/json\"}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "headers = {\"Content-Type\": \"application/octet-stream\"}\n",
        "\n",
        "data_file = prefix.joinpath(\"data\", \"raw\", \"images\", \"peacock.jpg\")\n",
        "test_sample = open(data_file, \"rb\").read()\n",
        "resp = requests.post(service.scoring_uri, data=test_sample, headers=headers)\n",
        "print(resp.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020-11-06T19:38:30,208456800+00:00 - iot-server/run \n2020-11-06T19:38:30,208483800+00:00 - gunicorn/run \n2020-11-06T19:38:30,209136300+00:00 - rsyslog/run \n2020-11-06T19:38:30,212484900+00:00 - triton/run \n2020-11-06T19:38:30,213213200+00:00 - nginx/run \n./run: line 14: exec: gunicorn: not found\n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.08 (build 15533555)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\n2020-11-06T19:38:30,266443500+00:00 - gunicorn/finish 127 0\n2020-11-06T19:38:30,268800800+00:00 - Exit code 127 is not normal. Killing image.\n\n"
          ]
        }
      ],
      "source": [
        "print(service.get_logs())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delete the webservice and the downloaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "service.delete()\n",
        "delete_triton_models(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next steps\n",
        "\n",
        "Try changing the deployment configuration to [deploy to Azure Kubernetes Service](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python) for higher availability and better scalability."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "Python 3.7.7 64-bit ('azureml': conda)",
      "display_name": "Python 3.7.7 64-bit ('azureml': conda)",
      "metadata": {
        "interpreter": {
          "hash": "53514593536e52de022f29ef618678eddccd581b6db5dc532e9838fb19203af5"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "name": "deploy-densenet-local",
    "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}