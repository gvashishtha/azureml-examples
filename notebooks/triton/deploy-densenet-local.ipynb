{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy to Triton Inference Server locally\n",
        "\n",
        "description: (preview) deploy an image classification model trained on densenet locally via Triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "ws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download model\n",
        "\n",
        "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "successfully downloaded model: densenet_onnx\n",
            "successfully downloaded model: bidaf-9\n"
          ]
        }
      ],
      "source": [
        "import git\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# get the root of the repo\n",
        "prefix = Path(git.Repo(\".\", search_parent_directories=True).working_tree_dir)\n",
        "\n",
        "# Enables us to import helper functions as Python modules\n",
        "path_to_insert = prefix.joinpath(\"code\", \"deployment\", \"triton\").__str__()\n",
        "if path_to_insert not in sys.path:\n",
        "    sys.path.insert(1, path_to_insert)\n",
        "\n",
        "from model_utils import download_triton_models, delete_triton_models\n",
        "\n",
        "\n",
        "download_triton_models(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register model\n",
        "\n",
        "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering model densenet-onnx-example\n",
            "Model(workspace=Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples'), name=densenet-onnx-example, id=densenet-onnx-example:531, version=531, tags={'area': 'Image classification', 'type': 'classification'}, properties={})\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model_path = prefix.joinpath(\"models\")\n",
        "\n",
        "model = Model.register(\n",
        "    model_path=model_path,\n",
        "    model_name=\"densenet-onnx-example\",\n",
        "    tags={\"area\": \"Image classification\", \"type\": \"classification\"},\n",
        "    description=\"Image classification trained on Imagenet Dataset\",\n",
        "    workspace=ws,\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy webservice\n",
        "\n",
        "In this case we deploy to the local compute, but for other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ng dependency tree...\n",
            "Reading state information...\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  distro-info-data iso-codes lsb-release powermgmt-base python-apt-common\n",
            "  python3-apt python3-software-properties unattended-upgrades\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following packages will be REMOVED:\n",
            "  software-properties-common*\n",
            "0 upgraded, 0 newly installed, 1 to remove and 55 not upgraded.\n",
            "After this operation, 201 kB disk space will be freed.\n",
            "(Reading database ... 21872 files and directories currently installed.)\n",
            "Removing software-properties-common (0.96.24.32.14) ...\n",
            "Processing triggers for dbus (1.12.2-1ubuntu1.2) ...\n",
            "(Reading database ... 21858 files and directories currently installed.)\n",
            "Purging configuration files for software-properties-common (0.96.24.32.14) ...\n",
            "Processing triggers for dbus (1.12.2-1ubuntu1.2) ...\n",
            "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 http://ppa.launchpad.net/adiscon/v8-stable/ubuntu bionic InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libunwind8 is already the newest version (1.2.1-8).\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "liblttng-ust0 is already the newest version (2.10.1-1).\n",
            "libxml++2.6-2v5 is already the newest version (2.40.1-2).\n",
            "runit is already the newest version (2.1.2-9.2ubuntu1).\n",
            "psmisc is already the newest version (23.1-1ubuntu0.1).\n",
            "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
            "nginx-light is already the newest version (1.14.0-0ubuntu1.7).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  distro-info-data iso-codes lsb-release powermgmt-base python-apt-common\n",
            "  python3-apt python3-software-properties unattended-upgrades\n",
            "Use 'apt autoremove' to remove them.\n",
            "Suggested packages:\n",
            "  rsyslog-mysql | rsyslog-pgsql rsyslog-doc rsyslog-relp rsyslog-elasticsearch\n",
            "  rsyslog-mmjsonparse rsyslog-imptcp rsyslog-gnutls rsyslog-openssl\n",
            "  rsyslog-udpspoof rsyslog-mmrm1stspace rsyslog-mmutf8fix rsyslog-kafka\n",
            "  rsyslog-omstdout apparmor\n",
            "The following packages will be REMOVED:\n",
            "  curl libcurl4\n",
            "The following NEW packages will be installed:\n",
            "  libcurl3\n",
            "The following packages will be upgraded:\n",
            "  rsyslog\n",
            "1 upgraded, 1 newly installed, 2 to remove and 54 not upgraded.\n",
            "Need to get 890 kB of archives.\n",
            "After this operation, 391 kB disk space will be freed.\n",
            "Get:1 http://ppa.launchpad.net/adiscon/v8-stable/ubuntu bionic/main amd64 rsyslog amd64 8.2010.0-0adiscon1bionic1 [676 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcurl3 amd64 7.58.0-2ubuntu2 [214 kB]\n",
            "Fetched 890 kB in 1s (1468 kB/s)\n",
            "(Reading database ... 21857 files and directories currently installed.)\n",
            "Removing curl (7.58.0-2ubuntu3.10) ...\n",
            "Removing libcurl4:amd64 (7.58.0-2ubuntu3.10) ...\n",
            "(Reading database ... 21844 files and directories currently installed.)\n",
            "Preparing to unpack .../rsyslog_8.2010.0-0adiscon1bionic1_amd64.deb ...\n",
            "Unpacking rsyslog (8.2010.0-0adiscon1bionic1) over (8.2008.0-0adiscon1bionic1) ...\n",
            "Selecting previously unselected package libcurl3:amd64.\n",
            "Preparing to unpack .../libcurl3_7.58.0-2ubuntu2_amd64.deb ...\n",
            "Unpacking libcurl3:amd64 (7.58.0-2ubuntu2) ...\n",
            "Setting up libcurl3:amd64 (7.58.0-2ubuntu2) ...\n",
            "Setting up rsyslog (8.2010.0-0adiscon1bionic1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of restart.\n",
            "Processing triggers for systemd (237-3ubuntu10.42) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "Removing intermediate container 62421797ea30\n",
            " ---> 8e40af8c6a91\n",
            "Step 14/20 : RUN cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && ln -sf /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && rm -f /etc/nginx/sites-enabled/default\n",
            " ---> Running in 2a6176b527fa\n",
            "Removing intermediate container 2a6176b527fa\n",
            " ---> ca06372f1a1b\n",
            "Step 15/20 : ENV SVDIR=/var/runit\n",
            " ---> Running in b29c10ed684f\n",
            "Removing intermediate container b29c10ed684f\n",
            " ---> 3994ee89560a\n",
            "Step 16/20 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\n",
            " ---> 79b0279763ca\n",
            "Step 17/20 : RUN if [ $SPARK_HOME ]; then /bin/bash -c '$SPARK_HOME/bin/spark-submit  /azureml-environment-setup/spark_cache.py'; fi\n",
            " ---> Running in eb9c1d37c19a\n",
            "Removing intermediate container eb9c1d37c19a\n",
            " ---> 5b4bdeceec0a\n",
            "Step 18/20 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
            " ---> Running in b56123164bde\n",
            "Removing intermediate container b56123164bde\n",
            " ---> 274c7e36bde1\n",
            "Step 19/20 : CMD [\"bash\"]\n",
            " ---> Running in 905c04ed0544\n",
            "Removing intermediate container 905c04ed0544\n",
            " ---> 799840b59b92\n",
            "Step 20/20 : EXPOSE 5001 8883 8888\n",
            " ---> Running in fa06b536e383\n",
            "Removing intermediate container fa06b536e383\n",
            " ---> 1ea13ab4e3d2\n",
            "Successfully built 1ea13ab4e3d2\n",
            "Successfully tagged 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_b4c36c0428c06431829990b6c34ef4dc:latest\n",
            "2020/11/06 19:52:31 Successfully executed container: acb_step_0\n",
            "2020/11/06 19:52:31 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
            "2020/11/06 19:52:31 Pushing image: 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_b4c36c0428c06431829990b6c34ef4dc:latest, attempt 1\n",
            "The push refers to repository [0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_b4c36c0428c06431829990b6c34ef4dc]\n",
            "ba28f7be7a64: Preparing\n",
            "cf1e7e7c5c64: Preparing\n",
            "2baa34a6f758: Preparing\n",
            "0fa5bfaa4150: Preparing\n",
            "2b85a9cf3b0b: Preparing\n",
            "f080beeb0d63: Preparing\n",
            "72a182f62fb0: Preparing\n",
            "094305a123c2: Preparing\n",
            "3dedce934be5: Preparing\n",
            "f2f5e0b92981: Preparing\n",
            "020f15d03188: Preparing\n",
            "41bcca9d7356: Preparing\n",
            "60a55b3a0fb3: Preparing\n",
            "e2a65dfaf981: Preparing\n",
            "ba453ad511de: Preparing\n",
            "14265ad8eb82: Preparing\n",
            "ffb531198263: Preparing\n",
            "767528c09e6f: Preparing\n",
            "521d1c6c0403: Preparing\n",
            "cc0fe7ad437f: Preparing\n",
            "d9c7fbf52fa6: Preparing\n",
            "fbc22e5b9b52: Preparing\n",
            "b9a711eecc9d: Preparing\n",
            "3d23030e5406: Preparing\n",
            "2bb763224424: Preparing\n",
            "8af27b6f81c4: Preparing\n",
            "c8948a865b2e: Preparing\n",
            "18e76b1e90da: Preparing\n",
            "b910ce715cf8: Preparing\n",
            "3b263b4195ff: Preparing\n",
            "05777f096096: Preparing\n",
            "14751c9e9be4: Preparing\n",
            "53c5d556a2ce: Preparing\n",
            "76539ba3a8f9: Preparing\n",
            "b7ff126c9912: Preparing\n",
            "16d39cc58c32: Preparing\n",
            "69ca13a8a6ad: Preparing\n",
            "715b5b90fc6a: Preparing\n",
            "2aefae9cd195: Preparing\n",
            "31b8fd860433: Preparing\n",
            "7f25196d29b9: Preparing\n",
            "6d85dc9d77d9: Preparing\n",
            "d7dc87a0f336: Preparing\n",
            "a42876287df5: Preparing\n",
            "32877aea68cd: Preparing\n",
            "3b1f1ccf7ee0: Preparing\n",
            "f6a451c2f8ad: Preparing\n",
            "fc052a1a6205: Preparing\n",
            "9642546ec635: Preparing\n",
            "31108b3c8971: Preparing\n",
            "dd623a2b059b: Preparing\n",
            "4e5530055b29: Preparing\n",
            "7fccf1b5f44c: Preparing\n",
            "9b50b917ef50: Preparing\n",
            "84ad3c4877ef: Preparing\n",
            "e606b71a5be7: Preparing\n",
            "c3fd4487ecb7: Preparing\n",
            "f11c5173a9ae: Preparing\n",
            "417dc169ba53: Preparing\n",
            "026b36b4dc3d: Preparing\n",
            "3445877f8a1f: Preparing\n",
            "996f146f5dbd: Preparing\n",
            "8682f9a74649: Preparing\n",
            "d3a6da143c91: Preparing\n",
            "83f4287e1f04: Preparing\n",
            "7ef368776582: Preparing\n",
            "76539ba3a8f9: Waiting\n",
            "b7ff126c9912: Waiting\n",
            "16d39cc58c32: Waiting\n",
            "69ca13a8a6ad: Waiting\n",
            "715b5b90fc6a: Waiting\n",
            "2aefae9cd195: Waiting\n",
            "31b8fd860433: Waiting\n",
            "7f25196d29b9: Waiting\n",
            "6d85dc9d77d9: Waiting\n",
            "d7dc87a0f336: Waiting\n",
            "a42876287df5: Waiting\n",
            "32877aea68cd: Waiting\n",
            "3b1f1ccf7ee0: Waiting\n",
            "f6a451c2f8ad: Waiting\n",
            "fc052a1a6205: Waiting\n",
            "9642546ec635: Waiting\n",
            "31108b3c8971: Waiting\n",
            "dd623a2b059b: Waiting\n",
            "4e5530055b29: Waiting\n",
            "7fccf1b5f44c: Waiting\n",
            "9b50b917ef50: Waiting\n",
            "84ad3c4877ef: Waiting\n",
            "e606b71a5be7: Waiting\n",
            "c3fd4487ecb7: Waiting\n",
            "f11c5173a9ae: Waiting\n",
            "417dc169ba53: Waiting\n",
            "026b36b4dc3d: Waiting\n",
            "3445877f8a1f: Waiting\n",
            "996f146f5dbd: Waiting\n",
            "8682f9a74649: Waiting\n",
            "d3a6da143c91: Waiting\n",
            "7ef368776582: Waiting\n",
            "767528c09e6f: Waiting\n",
            "521d1c6c0403: Waiting\n",
            "cc0fe7ad437f: Waiting\n",
            "d9c7fbf52fa6: Waiting\n",
            "fbc22e5b9b52: Waiting\n",
            "b9a711eecc9d: Waiting\n",
            "3d23030e5406: Waiting\n",
            "2bb763224424: Waiting\n",
            "8af27b6f81c4: Waiting\n",
            "c8948a865b2e: Waiting\n",
            "18e76b1e90da: Waiting\n",
            "b910ce715cf8: Waiting\n",
            "3b263b4195ff: Waiting\n",
            "05777f096096: Waiting\n",
            "14751c9e9be4: Waiting\n",
            "53c5d556a2ce: Waiting\n",
            "83f4287e1f04: Waiting\n",
            "f080beeb0d63: Waiting\n",
            "72a182f62fb0: Waiting\n",
            "094305a123c2: Waiting\n",
            "3dedce934be5: Waiting\n",
            "41bcca9d7356: Waiting\n",
            "60a55b3a0fb3: Waiting\n",
            "e2a65dfaf981: Waiting\n",
            "ba453ad511de: Waiting\n",
            "14265ad8eb82: Waiting\n",
            "ffb531198263: Waiting\n",
            "f2f5e0b92981: Waiting\n",
            "020f15d03188: Waiting\n",
            "2b85a9cf3b0b: Pushed\n",
            "ba28f7be7a64: Pushed\n",
            "2baa34a6f758: Pushed\n",
            "cf1e7e7c5c64: Pushed\n",
            "72a182f62fb0: Pushed\n",
            "0fa5bfaa4150: Pushed\n",
            "094305a123c2: Pushed\n",
            "f080beeb0d63: Pushed\n",
            "f2f5e0b92981: Pushed\n",
            "020f15d03188: Pushed\n",
            "41bcca9d7356: Pushed\n",
            "e2a65dfaf981: Pushed\n",
            "60a55b3a0fb3: Pushed\n",
            "ba453ad511de: Pushed\n",
            "14265ad8eb82: Pushed\n",
            "ffb531198263: Pushed\n",
            "521d1c6c0403: Pushed\n",
            "767528c09e6f: Pushed\n",
            "cc0fe7ad437f: Pushed\n",
            "b9a711eecc9d: Pushed\n",
            "3d23030e5406: Pushed\n",
            "2bb763224424: Pushed\n",
            "d9c7fbf52fa6: Pushed\n",
            "8af27b6f81c4: Pushed\n",
            "c8948a865b2e: Pushed\n",
            "fbc22e5b9b52: Pushed\n",
            "18e76b1e90da: Pushed\n",
            "05777f096096: Pushed\n",
            "14751c9e9be4: Pushed\n",
            "53c5d556a2ce: Pushed\n",
            "76539ba3a8f9: Pushed\n",
            "b7ff126c9912: Pushed\n",
            "16d39cc58c32: Pushed\n",
            "69ca13a8a6ad: Pushed\n",
            "2aefae9cd195: Pushed\n",
            "715b5b90fc6a: Pushed\n",
            "3dedce934be5: Pushed\n",
            "7f25196d29b9: Pushed\n",
            "6d85dc9d77d9: Pushed\n",
            "a42876287df5: Pushed\n",
            "32877aea68cd: Pushed\n",
            "d7dc87a0f336: Pushed\n",
            "3b1f1ccf7ee0: Pushed\n",
            "fc052a1a6205: Pushed\n",
            "9642546ec635: Pushed\n",
            "31108b3c8971: Pushed\n",
            "dd623a2b059b: Pushed\n",
            "31b8fd860433: Pushed\n",
            "7fccf1b5f44c: Pushed\n",
            "f6a451c2f8ad: Pushed\n",
            "9b50b917ef50: Pushed\n",
            "84ad3c4877ef: Pushed\n",
            "e606b71a5be7: Pushed\n",
            "c3fd4487ecb7: Pushed\n",
            "f11c5173a9ae: Pushed\n",
            "417dc169ba53: Pushed\n",
            "026b36b4dc3d: Pushed\n",
            "3445877f8a1f: Pushed\n",
            "8682f9a74649: Pushed\n",
            "d3a6da143c91: Pushed\n",
            "83f4287e1f04: Pushed\n",
            "7ef368776582: Pushed\n",
            "996f146f5dbd: Pushed\n",
            "3b263b4195ff: Pushed\n",
            "b910ce715cf8: Pushed\n",
            "4e5530055b29: Pushed\n",
            "latest: digest: sha256:c0e9a9d3d340afe2a510dc146117f5862337408e717f21eb691555fa7c1d8716 size: 14131\n",
            "2020/11/06 20:01:25 Successfully pushed image: 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_b4c36c0428c06431829990b6c34ef4dc:latest\n",
            "2020/11/06 20:01:25 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 390.155618)\n",
            "2020/11/06 20:01:25 Populating digests for step ID: acb_step_0...\n",
            "2020/11/06 20:01:28 Successfully populated digests for step ID: acb_step_0\n",
            "2020/11/06 20:01:28 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 534.513412)\n",
            "2020/11/06 20:01:28 The following dependencies were found:\n",
            "2020/11/06 20:01:28 \n",
            "- image:\n",
            "    registry: 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
            "    repository: azureml/azureml_b4c36c0428c06431829990b6c34ef4dc\n",
            "    tag: latest\n",
            "    digest: sha256:c0e9a9d3d340afe2a510dc146117f5862337408e717f21eb691555fa7c1d8716\n",
            "  runtime-dependency:\n",
            "    registry: mcr.microsoft.com\n",
            "    repository: azureml/aml-triton\n",
            "    tag: latest\n",
            "    digest: sha256:015907566598fa1dea941fb3b19a16495ae5ce83ad13b432db2bbc0b2448f545\n",
            "  buildtime-dependency:\n",
            "  - registry: mcr.microsoft.com\n",
            "    repository: azureml/o16n-base/python-assets\n",
            "    tag: latest\n",
            "    digest: sha256:2b5b27abf2384c07ab46f7fdcad8193b8ca673c47fec49a12ae2c3c359168c9c\n",
            "  git: {}\n",
            "\n",
            "\n",
            "Run ID: ca4f was successful after 15m33s\n",
            "Package creation Succeeded\n",
            "Logging into Docker registry 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
            "Logging into Docker registry 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
            "Building Docker image from Dockerfile...\n",
            "Step 1/5 : FROM 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_b4c36c0428c06431829990b6c34ef4dc\n",
            " ---> 1ea13ab4e3d2\n",
            "Step 2/5 : COPY azureml-app /var/azureml-app\n",
            " ---> 065f3928b00b\n",
            "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjY1NjA1NzVkLWZhMDYtNGU3ZC05NWZiLWY5NjJlNzRlZmQ3YSIsInJlc291cmNlR3JvdXBOYW1lIjoiYXp1cmVtbC1leGFtcGxlcyIsImFjY291bnROYW1lIjoiZGVmYXVsdCIsIndvcmtzcGFjZUlkIjoiMGUxNDk3NjQtMzcyMC00NjEwLWIwZjMtM2UzZjk3NDU0NGFjIn0sIm1vZGVscyI6e30sIm1vZGVsc0luZm8iOnt9fQ== | base64 --decode > /var/azureml-app/model_config_map.json\n",
            " ---> Running in d6aeeb813c49\n",
            " ---> 30bdad9fc73d\n",
            "Step 4/5 : RUN mv '/var/azureml-app/tmpk4xsufch.py' /var/azureml-app/main.py\n",
            " ---> Running in 1f957c1e73c7\n",
            " ---> 070a133d2905\n",
            "Step 5/5 : CMD [\"bash\",\"-c\",\"source activate '/opt/miniconda'; export AZUREML_CONDA_ENVIRONMENT_PATH=${AZUREML_CONDA_ENVIRONMENT_PATH:=$CONDA_PREFIX}; exec 'runsvdir' '/var/runit'\"]\n",
            " ---> Running in faedd616a253\n",
            " ---> cf8683221cba\n",
            "Successfully built cf8683221cba\n",
            "Successfully tagged triton-densenet-onnx-local30418:latest\n",
            "Starting Docker container...\n",
            "Docker container running.\n",
            "Checking container health...\n",
            "ERROR - Error: Container has crashed. Did your init method fail?\n",
            "\n",
            "\n",
            "Container Logs:\n",
            "2020-11-06T20:04:10,543881200+00:00 - gunicorn/run \n",
            "2020-11-06T20:04:10,549812200+00:00 - iot-server/run \n",
            "2020-11-06T20:04:10,549633600+00:00 - nginx/run \n",
            "2020-11-06T20:04:10,552942200+00:00 - triton/run \n",
            "2020-11-06T20:04:10,552511900+00:00 - rsyslog/run \n",
            "\n",
            "=============================\n",
            "== Triton Inference Server ==\n",
            "=============================\n",
            "\n",
            "NVIDIA Release 20.08 (build 15533555)\n",
            "\n",
            "Copyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n",
            "\n",
            "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\n",
            "NVIDIA modifications are covered by the license terms that apply to the underlying\n",
            "project or file.\n",
            "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
            "2020-11-06T20:04:10,716717600+00:00 - iot-server/finish 1 0\n",
            "2020-11-06T20:04:10,719164200+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
            "Starting gunicorn 19.9.0\n",
            "Listening at: http://127.0.0.1:31311 (27)\n",
            "Using worker: sync\n",
            "worker timeout is set to 300\n",
            "Booting worker with pid: 115\n",
            "find: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n",
            "\n",
            "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n",
            "   Use 'nvidia-docker run' to start this container; see\n",
            "   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n",
            "SPARK_HOME not set. Skipping PySpark Initialization.\n",
            "Initializing logger\n",
            "2020-11-06 20:04:11,843 | root | INFO | Starting up app insights client\n",
            "Starting up app insights client\n",
            "2020-11-06 20:04:11,843 | root | INFO | Starting up request id generator\n",
            "Starting up request id generator\n",
            "2020-11-06 20:04:11,843 | root | INFO | Starting up app insight hooks\n",
            "Starting up app insight hooks\n",
            "2020-11-06 20:04:11,843 | root | INFO | Invoking user's init function\n",
            "Invoking user's init function\n",
            "2020-11-06 20:04:11,861 | root | ERROR | User's init function failed\n",
            "User's init function failed\n",
            "2020-11-06 20:04:11,863 | root | ERROR | Encountered Exception Traceback (most recent call last):\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 163, in get_socket\n",
            "    return self._socket_queue.get(block=False)\n",
            "  File \"src/gevent/queue.py\", line 334, in gevent._gevent_cqueue.Queue.get\n",
            "  File \"src/gevent/queue.py\", line 349, in gevent._gevent_cqueue.Queue.get\n",
            "  File \"src/gevent/queue.py\", line 318, in gevent._gevent_cqueue.Queue._Queue__get_or_peek\n",
            "_queue.Empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/var/azureml-server/aml_blueprint.py\", line 163, in register\n",
            "    main.init()\n",
            "  File \"/var/azureml-app/main.py\", line 35, in init\n",
            "    driver_module.init()\n",
            "  File \"/var/azureml-app/triton/score_densenet.py\", line 85, in init\n",
            "    session = InferenceSession(path_or_bytes='densenet_onnx')\n",
            "  File \"/var/azureml-app/triton/onnxruntimetriton.py\", line 11, in __init__\n",
            "    model_metadata = self.client.get_model_metadata(model_name=path_or_bytes)\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/tritonclient/http/__init__.py\", line 494, in get_model_metadata\n",
            "    query_params=query_params)\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/tritonclient/http/__init__.py\", line 258, in _get\n",
            "    response = self._client_stub.get(request_uri)\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/client.py\", line 266, in get\n",
            "    return self.request(METHOD_GET, request_uri, headers=headers)\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/client.py\", line 226, in request\n",
            "    sock = self._connection_pool.get_socket()\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 166, in get_socket\n",
            "    return self._create_socket()\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 127, in _create_socket\n",
            "    raise first_error\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 114, in _create_socket\n",
            "    sock = self._connect_socket(sock, sock_info[-1])\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 136, in _connect_socket\n",
            "    sock.connect(address)\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/gevent/_socket3.py\", line 407, in connect\n",
            "    raise error(err, strerror(err))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "Encountered Exception Traceback (most recent call last):\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 163, in get_socket\n",
            "    return self._socket_queue.get(block=False)\n",
            "  File \"src/gevent/queue.py\", line 334, in gevent._gevent_cqueue.Queue.get\n",
            "  File \"src/gevent/queue.py\", line 349, in gevent._gevent_cqueue.Queue.get\n",
            "  File \"src/gevent/queue.py\", line 318, in gevent._gevent_cqueue.Queue._Queue__get_or_peek\n",
            "_queue.Empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/var/azureml-server/aml_blueprint.py\", line 163, in register\n",
            "    main.init()\n",
            "  File \"/var/azureml-app/main.py\", line 35, in init\n",
            "    driver_module.init()\n",
            "  File \"/var/azureml-app/triton/score_densenet.py\", line 85, in init\n",
            "    session = InferenceSession(path_or_bytes='densenet_onnx')\n",
            "  File \"/var/azureml-app/triton/onnxruntimetriton.py\", line 11, in __init__\n",
            "    model_metadata = self.client.get_model_metadata(model_name=path_or_bytes)\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/tritonclient/http/__init__.py\", line 494, in get_model_metadata\n",
            "    query_params=query_params)\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/tritonclient/http/__init__.py\", line 258, in _get\n",
            "    response = self._client_stub.get(request_uri)\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/client.py\", line 266, in get\n",
            "    return self.request(METHOD_GET, request_uri, headers=headers)\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/client.py\", line 226, in request\n",
            "    sock = self._connection_pool.get_socket()\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 166, in get_socket\n",
            "    return self._create_socket()\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 127, in _create_socket\n",
            "    raise first_error\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 114, in _create_socket\n",
            "    sock = self._connect_socket(sock, sock_info[-1])\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 136, in _connect_socket\n",
            "    sock.connect(address)\n",
            "  File \"/opt/miniconda/lib/python3.7/site-packages/gevent/_socket3.py\", line 407, in connect\n",
            "    raise error(err, strerror(err))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "Worker exiting (pid: 115)\n",
            "/opt/miniconda/lib/python3.7/site-packages/tritonhttpclient/__init__.py:33: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
            "  \"`tritonclient.http`\", DeprecationWarning)\n",
            "Shutting down: Master\n",
            "Reason: Worker failed to boot.\n",
            "2020-11-06T20:04:11,985972300+00:00 - gunicorn/finish 3 0\n",
            "2020-11-06T20:04:11,988494500+00:00 - Exit code 3 is not normal. Killing image.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "WebserviceException",
          "evalue": "WebserviceException:\n\tMessage: Error: Container has crashed. Did your init method fail?\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Error: Container has crashed. Did your init method fail?\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-64546c8dea3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_deployment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n\u001b[1;32m     70\u001b[0m                                           logger=module_logger)\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mwait_for_deployment\u001b[0;34m(self, show_output)\u001b[0m\n\u001b[1;32m    601\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                                    \u001b[0mhealth_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_base_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                                    cleanup_if_failed=False)\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalWebservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATE_RUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/_model_management/_util.py\u001b[0m in \u001b[0;36mcontainer_health_check\u001b[0;34m(docker_port, container, health_url, cleanup_if_failed)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;31m# The container has started and crashed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             _raise_for_container_failure(container, cleanup_if_failed,\n\u001b[0;32m--> 741\u001b[0;31m                                          'Error: Container has crashed. Did your init method fail?')\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;31m# The container hasn't crashed, so try to ping the health endpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/_model_management/_util.py\u001b[0m in \u001b[0;36m_raise_for_container_failure\u001b[0;34m(container, cleanup, message)\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0mcleanup_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mWebserviceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Error: Container has crashed. Did your init method fail?\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Error: Container has crashed. Did your init method fail?\"\n    }\n}"
          ]
        }
      ],
      "source": [
        "from azureml.core.webservice import LocalWebservice\n",
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.model import InferenceConfig\n",
        "from random import randint\n",
        "\n",
        "service_name = \"triton-densenet-onnx-local\" + str(randint(10000, 99999))\\\n",
        "\n",
        "# This doesn't work because the install order is not respected\n",
        "# env = Environment.get(ws, \"AzureML-Triton\").clone(\"triton-example\")\n",
        "\n",
        "# for pip_package in [\"pillow\", \"nvidia-pyindex\", \"tritonclient[http]\"]:\n",
        "#     env.python.conda_dependencies.add_pip_package(pip_package)\n",
        "\n",
        "\n",
        "env = Environment(\"triton-example\")\n",
        "env.docker.base_image = None\n",
        "env.docker.base_dockerfile=prefix.joinpath(\"notebooks\", \"triton\", \"docker\", \"Dockerfile\")\n",
        "env.python.user_managed_dependencies=True\n",
        "env.python.interpreter_path='/opt/miniconda/bin/python'\n",
        "env.inferencing_stack_version='latest'\n",
        "\n",
        "env.environment_variables['WORKER_COUNT']='1'\n",
        "\n",
        "inference_config = InferenceConfig(\n",
        "    # this entry script is where we dispatch a call to the Triton server\n",
        "    entry_script=\"score_densenet.py\",\n",
        "    source_directory=prefix.joinpath(\"code\", \"deployment\", \"triton\"),\n",
        "    environment=env,\n",
        ")\n",
        "\n",
        "config = LocalWebservice.deploy_configuration(port=6789)\n",
        "\n",
        "service = Model.deploy(\n",
        "    workspace=ws,\n",
        "    name=service_name,\n",
        "    models=[model],\n",
        "    inference_config=inference_config,\n",
        "    deployment_config=config,\n",
        "    overwrite=True,\n",
        ")\n",
        "\n",
        "service.wait_for_deployment(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Container has been successfully cleaned up.\n",
            "Starting Docker container...\n",
            "Docker container running.\n"
          ]
        }
      ],
      "source": [
        "service.reload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the webservice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"message\": \"Expects Content-Type to be application/json\"}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "headers = {\"Content-Type\": \"application/octet-stream\"}\n",
        "\n",
        "data_file = prefix.joinpath(\"data\", \"raw\", \"images\", \"peacock.jpg\")\n",
        "test_sample = open(data_file, \"rb\").read()\n",
        "resp = requests.post(service.scoring_uri, data=test_sample, headers=headers)\n",
        "print(resp.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020-11-06T20:04:10,543881200+00:00 - gunicorn/run \n2020-11-06T20:04:10,549812200+00:00 - iot-server/run \n2020-11-06T20:04:10,549633600+00:00 - nginx/run \n2020-11-06T20:04:10,552942200+00:00 - triton/run \n2020-11-06T20:04:10,552511900+00:00 - rsyslog/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.08 (build 15533555)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2020-11-06T20:04:10,716717600+00:00 - iot-server/finish 1 0\n2020-11-06T20:04:10,719164200+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 19.9.0\nListening at: http://127.0.0.1:31311 (27)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 115\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\nSPARK_HOME not set. Skipping PySpark Initialization.\nInitializing logger\n2020-11-06 20:04:11,843 | root | INFO | Starting up app insights client\nStarting up app insights client\n2020-11-06 20:04:11,843 | root | INFO | Starting up request id generator\nStarting up request id generator\n2020-11-06 20:04:11,843 | root | INFO | Starting up app insight hooks\nStarting up app insight hooks\n2020-11-06 20:04:11,843 | root | INFO | Invoking user's init function\nInvoking user's init function\n2020-11-06 20:04:11,861 | root | ERROR | User's init function failed\nUser's init function failed\n2020-11-06 20:04:11,863 | root | ERROR | Encountered Exception Traceback (most recent call last):\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 163, in get_socket\n    return self._socket_queue.get(block=False)\n  File \"src/gevent/queue.py\", line 334, in gevent._gevent_cqueue.Queue.get\n  File \"src/gevent/queue.py\", line 349, in gevent._gevent_cqueue.Queue.get\n  File \"src/gevent/queue.py\", line 318, in gevent._gevent_cqueue.Queue._Queue__get_or_peek\n_queue.Empty\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/azureml-server/aml_blueprint.py\", line 163, in register\n    main.init()\n  File \"/var/azureml-app/main.py\", line 35, in init\n    driver_module.init()\n  File \"/var/azureml-app/triton/score_densenet.py\", line 85, in init\n    session = InferenceSession(path_or_bytes='densenet_onnx')\n  File \"/var/azureml-app/triton/onnxruntimetriton.py\", line 11, in __init__\n    model_metadata = self.client.get_model_metadata(model_name=path_or_bytes)\n  File \"/opt/miniconda/lib/python3.7/site-packages/tritonclient/http/__init__.py\", line 494, in get_model_metadata\n    query_params=query_params)\n  File \"/opt/miniconda/lib/python3.7/site-packages/tritonclient/http/__init__.py\", line 258, in _get\n    response = self._client_stub.get(request_uri)\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/client.py\", line 266, in get\n    return self.request(METHOD_GET, request_uri, headers=headers)\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/client.py\", line 226, in request\n    sock = self._connection_pool.get_socket()\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 166, in get_socket\n    return self._create_socket()\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 127, in _create_socket\n    raise first_error\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 114, in _create_socket\n    sock = self._connect_socket(sock, sock_info[-1])\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 136, in _connect_socket\n    sock.connect(address)\n  File \"/opt/miniconda/lib/python3.7/site-packages/gevent/_socket3.py\", line 407, in connect\n    raise error(err, strerror(err))\nConnectionRefusedError: [Errno 111] Connection refused\n\nEncountered Exception Traceback (most recent call last):\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 163, in get_socket\n    return self._socket_queue.get(block=False)\n  File \"src/gevent/queue.py\", line 334, in gevent._gevent_cqueue.Queue.get\n  File \"src/gevent/queue.py\", line 349, in gevent._gevent_cqueue.Queue.get\n  File \"src/gevent/queue.py\", line 318, in gevent._gevent_cqueue.Queue._Queue__get_or_peek\n_queue.Empty\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/azureml-server/aml_blueprint.py\", line 163, in register\n    main.init()\n  File \"/var/azureml-app/main.py\", line 35, in init\n    driver_module.init()\n  File \"/var/azureml-app/triton/score_densenet.py\", line 85, in init\n    session = InferenceSession(path_or_bytes='densenet_onnx')\n  File \"/var/azureml-app/triton/onnxruntimetriton.py\", line 11, in __init__\n    model_metadata = self.client.get_model_metadata(model_name=path_or_bytes)\n  File \"/opt/miniconda/lib/python3.7/site-packages/tritonclient/http/__init__.py\", line 494, in get_model_metadata\n    query_params=query_params)\n  File \"/opt/miniconda/lib/python3.7/site-packages/tritonclient/http/__init__.py\", line 258, in _get\n    response = self._client_stub.get(request_uri)\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/client.py\", line 266, in get\n    return self.request(METHOD_GET, request_uri, headers=headers)\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/client.py\", line 226, in request\n    sock = self._connection_pool.get_socket()\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 166, in get_socket\n    return self._create_socket()\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 127, in _create_socket\n    raise first_error\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 114, in _create_socket\n    sock = self._connect_socket(sock, sock_info[-1])\n  File \"/opt/miniconda/lib/python3.7/site-packages/geventhttpclient/connectionpool.py\", line 136, in _connect_socket\n    sock.connect(address)\n  File \"/opt/miniconda/lib/python3.7/site-packages/gevent/_socket3.py\", line 407, in connect\n    raise error(err, strerror(err))\nConnectionRefusedError: [Errno 111] Connection refused\n\nWorker exiting (pid: 115)\n/opt/miniconda/lib/python3.7/site-packages/tritonhttpclient/__init__.py:33: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n  \"`tritonclient.http`\", DeprecationWarning)\nShutting down: Master\nReason: Worker failed to boot.\n2020-11-06T20:04:11,985972300+00:00 - gunicorn/finish 3 0\n2020-11-06T20:04:11,988494500+00:00 - Exit code 3 is not normal. Killing image.\n\n"
          ]
        }
      ],
      "source": [
        "print(service.get_logs())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delete the webservice and the downloaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "service.delete()\n",
        "delete_triton_models(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next steps\n",
        "\n",
        "Try changing the deployment configuration to [deploy to Azure Kubernetes Service](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python) for higher availability and better scalability."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "Python 3.7.7 64-bit ('azureml': conda)",
      "display_name": "Python 3.7.7 64-bit ('azureml': conda)",
      "metadata": {
        "interpreter": {
          "hash": "53514593536e52de022f29ef618678eddccd581b6db5dc532e9838fb19203af5"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "name": "deploy-densenet-local",
    "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}