{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy to Triton Inference Server locally\n",
        "\n",
        "description: (preview) deploy an image classification model trained on densenet locally via Triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "ws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download model\n",
        "\n",
        "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "successfully downloaded model: densenet_onnx\n",
            "successfully downloaded model: bidaf-9\n"
          ]
        }
      ],
      "source": [
        "import git\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# get the root of the repo\n",
        "prefix = Path(git.Repo(\".\", search_parent_directories=True).working_tree_dir)\n",
        "\n",
        "# Enables us to import helper functions as Python modules\n",
        "path_to_insert = prefix.joinpath(\"code\", \"deployment\", \"triton\").__str__()\n",
        "if path_to_insert not in sys.path:\n",
        "    sys.path.insert(1, path_to_insert)\n",
        "\n",
        "from model_utils import download_triton_models, delete_triton_models\n",
        "\n",
        "\n",
        "download_triton_models(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register model\n",
        "\n",
        "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering model densenet-onnx-example\n",
            "Model(workspace=Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples'), name=densenet-onnx-example, id=densenet-onnx-example:510, version=510, tags={'area': 'Image classification', 'type': 'classification'}, properties={})\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model_path = prefix.joinpath(\"models\")\n",
        "\n",
        "model = Model.register(\n",
        "    model_path=model_path,\n",
        "    model_name=\"densenet-onnx-example\",\n",
        "    tags={\"area\": \"Image classification\", \"type\": \"classification\"},\n",
        "    description=\"Image classification trained on Imagenet Dataset\",\n",
        "    workspace=ws,\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy webservice\n",
        "\n",
        "In this case we deploy to the local compute, but for other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> 13372a84359a\n",
            "Step 5/20 : RUN pip install nvidia-pyindex\n",
            " ---> Running in 185dea9994f1\n",
            "Collecting nvidia-pyindex\n",
            "  Downloading https://files.pythonhosted.org/packages/64/4c/dd413559179536b9b7247f15bf968f7e52b5f8c1d2183ceb3d5ea9284776/nvidia-pyindex-1.0.5.tar.gz\n",
            "Building wheels for collected packages: nvidia-pyindex\n",
            "  Building wheel for nvidia-pyindex (setup.py): started\n",
            "  Building wheel for nvidia-pyindex (setup.py): finished with status 'done'\n",
            "  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.5-cp37-none-any.whl size=4171 sha256=a945ea1c16919e1ff3e98e3cf5fb8288e241cd97a8d4ff7bbd0f7f3120e950a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/09/ce/acc25e8cebda16e490a9610b11f98c111e761d15090e9fb9a3\n",
            "Successfully built nvidia-pyindex\n",
            "Installing collected packages: nvidia-pyindex\n",
            "Successfully installed nvidia-pyindex-1.0.5\n",
            "Removing intermediate container 185dea9994f1\n",
            " ---> 0a471a54dab8\n",
            "Step 6/20 : RUN pip install tritonclient[http]\n",
            " ---> Running in ded100923b55\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting tritonclient[http]\n",
            "  Downloading https://developer.download.nvidia.com/compute/redist/tritonclient/tritonclient-2.4.0-py3-none-manylinux1_x86_64.whl (5.9MB)\n",
            "Collecting python-rapidjson>=0.9.1\n",
            "  Downloading https://files.pythonhosted.org/packages/67/a8/8decef6477075b5f14f789a06781daac3bd0bb72573c21f9f86b6aa5eed0/python_rapidjson-0.9.3-cp37-cp37m-manylinux2010_x86_64.whl (1.4MB)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /opt/miniconda/lib/python3.7/site-packages (from tritonclient[http]) (1.19.4)\n",
            "Collecting geventhttpclient>=1.4.4; extra == \"http\"\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/13/65c460301641c6b1a73f81fdc2933f8a194874c4e2e3d7077926093937b4/geventhttpclient-1.4.4-cp37-cp37m-manylinux2010_x86_64.whl (77kB)\n",
            "Collecting gevent>=0.13\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/a0/acbe4aecc341cc38641e82e2c12497f9ff4b621730e40c0c0411ad867a90/gevent-20.9.0-cp37-cp37m-manylinux2010_x86_64.whl (5.5MB)\n",
            "Requirement already satisfied: certifi in /opt/miniconda/lib/python3.7/site-packages (from geventhttpclient>=1.4.4; extra == \"http\"->tritonclient[http]) (2019.9.11)\n",
            "Requirement already satisfied: six in /opt/miniconda/lib/python3.7/site-packages (from geventhttpclient>=1.4.4; extra == \"http\"->tritonclient[http]) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /opt/miniconda/lib/python3.7/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4; extra == \"http\"->tritonclient[http]) (41.4.0)\n",
            "Collecting zope.interface\n",
            "  Downloading https://files.pythonhosted.org/packages/27/44/39859ae84c11152fcd2d96666547f1b3886b826105857bdc709c12249891/zope.interface-5.1.2-cp37-cp37m-manylinux2010_x86_64.whl (237kB)\n",
            "Collecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Collecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/34/2c4ea49f9d41aaf664f72abb10880a56d3560398b74b3936055579188836/greenlet-0.4.17-cp37-cp37m-manylinux1_x86_64.whl (45kB)\n",
            "Installing collected packages: python-rapidjson, zope.interface, zope.event, greenlet, gevent, geventhttpclient, tritonclient\n",
            "Successfully installed gevent-20.9.0 geventhttpclient-1.4.4 greenlet-0.4.17 python-rapidjson-0.9.3 tritonclient-2.4.0 zope.event-4.5.0 zope.interface-5.1.2\n",
            "Removing intermediate container ded100923b55\n",
            " ---> 9260270ced01\n",
            "Step 7/20 : USER root\n",
            " ---> Running in 1a66ec339048\n",
            "Removing intermediate container 1a66ec339048\n",
            " ---> 81e9b3c898c1\n",
            "Step 8/20 : RUN mkdir -p $HOME/.cache\n",
            " ---> Running in 863e21dc9748\n",
            "Removing intermediate container 863e21dc9748\n",
            " ---> 86d9ad8c16de\n",
            "Step 9/20 : WORKDIR /\n",
            " ---> Running in 43b5a7b6b42c\n",
            "Removing intermediate container 43b5a7b6b42c\n",
            " ---> afbc18c6d973\n",
            "Step 10/20 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
            " ---> 26ba339c036c\n",
            "Step 11/20 : RUN true\n",
            " ---> Running in 03b90f11a80e\n",
            "Removing intermediate container 03b90f11a80e\n",
            " ---> 95616cbedbf0\n",
            "Step 12/20 : COPY --from=inferencing-assets /artifacts /var/\n",
            " ---> 125e0a270982\n",
            "Step 13/20 : RUN /var/requirements/install_system_requirements.sh\n",
            " ---> Running in 1ee169b4aeaa\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://packages.microsoft.com/ubuntu/18.04/prod bionic InRelease [4002 B]\n",
            "Get:3 http://ppa.launchpad.net/adiscon/v8-stable/ubuntu bionic InRelease [15.4 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:6 https://packages.microsoft.com/ubuntu/18.04/prod bionic/main amd64 Packages [157 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1353 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1750 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [213 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.4 kB]\n",
            "Get:12 http://ppa.launchpad.net/adiscon/v8-stable/ubuntu bionic/main amd64 Packages [8282 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [45.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2118 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [239 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2167 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]\n",
            "Fetched 8361 kB in 2s (4176 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  software-properties-common\n",
            "0 upgraded, 1 newly installed, 0 to remove and 64 not upgraded.\n",
            "Need to get 10.1 kB of archives.\n",
            "After this operation, 201 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 software-properties-common all 0.96.24.32.14 [10.1 kB]\n",
            "Fetched 10.1 kB in 0s (64.0 kB/s)\n",
            "Selecting previously unselected package software-properties-common.\n",
            "(Reading database ... 21902 files and directories currently installed.)\n",
            "Preparing to unpack .../software-properties-common_0.96.24.32.14_all.deb ...\n",
            "Unpacking software-properties-common (0.96.24.32.14) ...\n",
            "Setting up software-properties-common (0.96.24.32.14) ...\n",
            "Processing triggers for dbus (1.12.2-1ubuntu1.2) ...\n",
            "Hit:1 https://packages.microsoft.com/ubuntu/18.04/prod bionic InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:4 http://ppa.launchpad.net/adiscon/v8-stable/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  distro-info-data iso-codes lsb-release powermgmt-base python-apt-common\n",
            "  python3-apt python3-software-properties unattended-upgrades\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following packages will be REMOVED:\n",
            "  software-properties-common*\n",
            "0 upgraded, 0 newly installed, 1 to remove and 64 not upgraded.\n",
            "After this operation, 201 kB disk space will be freed.\n",
            "(Reading database ... 21917 files and directories currently installed.)\n",
            "Removing software-properties-common (0.96.24.32.14) ...\n",
            "Processing triggers for dbus (1.12.2-1ubuntu1.2) ...\n",
            "(Reading database ... 21903 files and directories currently installed.)\n",
            "Purging configuration files for software-properties-common (0.96.24.32.14) ...\n",
            "Processing triggers for dbus (1.12.2-1ubuntu1.2) ...\n",
            "Hit:1 https://packages.microsoft.com/ubuntu/18.04/prod bionic InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:3 http://ppa.launchpad.net/adiscon/v8-stable/ubuntu bionic InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libunwind8 is already the newest version (1.2.1-8).\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "liblttng-ust0 is already the newest version (2.10.1-1).\n",
            "libxml++2.6-2v5 is already the newest version (2.40.1-2).\n",
            "runit is already the newest version (2.1.2-9.2ubuntu1).\n",
            "psmisc is already the newest version (23.1-1ubuntu0.1).\n",
            "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
            "nginx-light is already the newest version (1.14.0-0ubuntu1.7).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  distro-info-data iso-codes lsb-release powermgmt-base python-apt-common\n",
            "  python3-apt python3-software-properties unattended-upgrades\n",
            "Use 'apt autoremove' to remove them.\n",
            "Suggested packages:\n",
            "  rsyslog-mysql | rsyslog-pgsql rsyslog-doc rsyslog-relp rsyslog-elasticsearch\n",
            "  rsyslog-mmjsonparse rsyslog-imptcp rsyslog-gnutls rsyslog-openssl\n",
            "  rsyslog-udpspoof rsyslog-mmrm1stspace rsyslog-mmutf8fix rsyslog-kafka\n",
            "  rsyslog-omstdout apparmor\n",
            "The following packages will be REMOVED:\n",
            "  curl libcurl4\n",
            "The following NEW packages will be installed:\n",
            "  libcurl3\n",
            "The following packages will be upgraded:\n",
            "  rsyslog\n",
            "1 upgraded, 1 newly installed, 2 to remove and 63 not upgraded.\n",
            "Need to get 890 kB of archives.\n",
            "After this operation, 391 kB disk space will be freed.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcurl3 amd64 7.58.0-2ubuntu2 [214 kB]\n",
            "Get:2 http://ppa.launchpad.net/adiscon/v8-stable/ubuntu bionic/main amd64 rsyslog amd64 8.2010.0-0adiscon1bionic1 [676 kB]\n",
            "Fetched 890 kB in 1s (1444 kB/s)\n",
            "(Reading database ... 21902 files and directories currently installed.)\n",
            "Removing curl (7.58.0-2ubuntu3.10) ...\n",
            "Removing libcurl4:amd64 (7.58.0-2ubuntu3.10) ...\n",
            "(Reading database ... 21889 files and directories currently installed.)\n",
            "Preparing to unpack .../rsyslog_8.2010.0-0adiscon1bionic1_amd64.deb ...\n",
            "Unpacking rsyslog (8.2010.0-0adiscon1bionic1) over (8.2008.0-0adiscon1bionic1) ...\n",
            "Selecting previously unselected package libcurl3:amd64.\n",
            "Preparing to unpack .../libcurl3_7.58.0-2ubuntu2_amd64.deb ...\n",
            "Unpacking libcurl3:amd64 (7.58.0-2ubuntu2) ...\n",
            "Setting up libcurl3:amd64 (7.58.0-2ubuntu2) ...\n",
            "Setting up rsyslog (8.2010.0-0adiscon1bionic1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of restart.\n",
            "Processing triggers for systemd (237-3ubuntu10.42) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Removing intermediate container 1ee169b4aeaa\n",
            " ---> d3fefaef87b1\n",
            "Step 14/20 : RUN cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && ln -sf /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && rm -f /etc/nginx/sites-enabled/default\n",
            " ---> Running in 201830518a17\n",
            "Removing intermediate container 201830518a17\n",
            " ---> d3279338b3f8\n",
            "Step 15/20 : ENV SVDIR=/var/runit\n",
            " ---> Running in d6b754732ad0\n",
            "Removing intermediate container d6b754732ad0\n",
            " ---> ceedbda3e744\n",
            "Step 16/20 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\n",
            " ---> 4a3b8df86ad3\n",
            "Step 17/20 : RUN if [ $SPARK_HOME ]; then /bin/bash -c '$SPARK_HOME/bin/spark-submit  /azureml-environment-setup/spark_cache.py'; fi\n",
            " ---> Running in e4be871284c6\n",
            "Removing intermediate container e4be871284c6\n",
            " ---> 8f6170d1f109\n",
            "Step 18/20 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
            " ---> Running in 40cea6581140\n",
            "Removing intermediate container 40cea6581140\n",
            " ---> c4c9a123e2ef\n",
            "Step 19/20 : CMD [\"bash\"]\n",
            " ---> Running in ff6d1291ee47\n",
            "Removing intermediate container ff6d1291ee47\n",
            " ---> 510fd41cd843\n",
            "Step 20/20 : EXPOSE 5001 8883 8888\n",
            " ---> Running in 2d4a719b7e04\n",
            "Removing intermediate container 2d4a719b7e04\n",
            " ---> 52105735a82f\n",
            "Successfully built 52105735a82f\n",
            "Successfully tagged 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_c10f0953bc1e7da67ed17f33ab58f449:latest\n",
            "2020/11/05 03:12:48 Successfully executed container: acb_step_0\n",
            "2020/11/05 03:12:48 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
            "2020/11/05 03:12:48 Pushing image: 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_c10f0953bc1e7da67ed17f33ab58f449:latest, attempt 1\n",
            "The push refers to repository [0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_c10f0953bc1e7da67ed17f33ab58f449]\n",
            "4274592cc5a6: Preparing\n",
            "f87ce5b54ede: Preparing\n",
            "3b32d76aaeac: Preparing\n",
            "90f0739e864f: Preparing\n",
            "2dfe077478e7: Preparing\n",
            "ff925d82746c: Preparing\n",
            "e504c5c14c58: Preparing\n",
            "5e7be15cc7ac: Preparing\n",
            "add1badbdc49: Preparing\n",
            "915acdc59dc7: Preparing\n",
            "fe58db7e29c9: Preparing\n",
            "73361934b445: Preparing\n",
            "1aa0fa3a5852: Preparing\n",
            "f1b6c74c4ba6: Preparing\n",
            "45f95fb7a389: Preparing\n",
            "a1aa70b7d447: Preparing\n",
            "a9e218674e04: Preparing\n",
            "5a61e03e9290: Preparing\n",
            "a02c84464441: Preparing\n",
            "96a152209e9e: Preparing\n",
            "88a6021ce6f5: Preparing\n",
            "8fb506f58648: Preparing\n",
            "da6ea20315f6: Preparing\n",
            "b580f8f66dce: Preparing\n",
            "b0f781117251: Preparing\n",
            "9db1bef051b5: Preparing\n",
            "de7dac334f31: Preparing\n",
            "b23d47b63e15: Preparing\n",
            "614c3f8a359c: Preparing\n",
            "ae9158965e0c: Preparing\n",
            "a47bd7f6fa0a: Preparing\n",
            "2b3fcd0d7124: Preparing\n",
            "ccd31c9fc36f: Preparing\n",
            "5277acf97db7: Preparing\n",
            "7ae4f18769b2: Preparing\n",
            "a4aa8b6c47b3: Preparing\n",
            "14f6719c9a9f: Preparing\n",
            "603726fa9d3f: Preparing\n",
            "9202e74cdbdb: Preparing\n",
            "7fd727c3ea0d: Preparing\n",
            "5709c385e246: Preparing\n",
            "ccf068a4d377: Preparing\n",
            "0bfaec991053: Preparing\n",
            "5428ff5c945d: Preparing\n",
            "461f1a8abdb4: Preparing\n",
            "d7d0183824f6: Preparing\n",
            "9d76323a95cf: Preparing\n",
            "59f16337b409: Preparing\n",
            "c8a7f9a65aad: Preparing\n",
            "a33a567a8065: Preparing\n",
            "0e38537de292: Preparing\n",
            "ed90e4663409: Preparing\n",
            "4f31588a31be: Preparing\n",
            "89c75506f8aa: Preparing\n",
            "2b9bb5b31e2a: Preparing\n",
            "049699798ae6: Preparing\n",
            "3e70085060ca: Preparing\n",
            "6b481735d372: Preparing\n",
            "b7d470eb74cc: Preparing\n",
            "96c055a01732: Preparing\n",
            "9b1bc2360477: Preparing\n",
            "c40a36ccac9e: Preparing\n",
            "ddc500d84994: Preparing\n",
            "c64c52ea2c16: Preparing\n",
            "5930c9e5703f: Preparing\n",
            "b187ff70b2e4: Preparing\n",
            "5277acf97db7: Waiting\n",
            "7ae4f18769b2: Waiting\n",
            "a4aa8b6c47b3: Waiting\n",
            "14f6719c9a9f: Waiting\n",
            "603726fa9d3f: Waiting\n",
            "9202e74cdbdb: Waiting\n",
            "7fd727c3ea0d: Waiting\n",
            "5709c385e246: Waiting\n",
            "ccf068a4d377: Waiting\n",
            "0bfaec991053: Waiting\n",
            "5428ff5c945d: Waiting\n",
            "461f1a8abdb4: Waiting\n",
            "d7d0183824f6: Waiting\n",
            "9d76323a95cf: Waiting\n",
            "59f16337b409: Waiting\n",
            "c8a7f9a65aad: Waiting\n",
            "a33a567a8065: Waiting\n",
            "0e38537de292: Waiting\n",
            "ed90e4663409: Waiting\n",
            "4f31588a31be: Waiting\n",
            "89c75506f8aa: Waiting\n",
            "2b9bb5b31e2a: Waiting\n",
            "049699798ae6: Waiting\n",
            "3e70085060ca: Waiting\n",
            "6b481735d372: Waiting\n",
            "b7d470eb74cc: Waiting\n",
            "96c055a01732: Waiting\n",
            "9b1bc2360477: Waiting\n",
            "c40a36ccac9e: Waiting\n",
            "ddc500d84994: Waiting\n",
            "c64c52ea2c16: Waiting\n",
            "b187ff70b2e4: Waiting\n",
            "5a61e03e9290: Waiting\n",
            "a02c84464441: Waiting\n",
            "ff925d82746c: Waiting\n",
            "e504c5c14c58: Waiting\n",
            "96a152209e9e: Waiting\n",
            "5e7be15cc7ac: Waiting\n",
            "88a6021ce6f5: Waiting\n",
            "add1badbdc49: Waiting\n",
            "915acdc59dc7: Waiting\n",
            "8fb506f58648: Waiting\n",
            "da6ea20315f6: Waiting\n",
            "fe58db7e29c9: Waiting\n",
            "73361934b445: Waiting\n",
            "b580f8f66dce: Waiting\n",
            "1aa0fa3a5852: Waiting\n",
            "b0f781117251: Waiting\n",
            "f1b6c74c4ba6: Waiting\n",
            "9db1bef051b5: Waiting\n",
            "de7dac334f31: Waiting\n",
            "45f95fb7a389: Waiting\n",
            "a1aa70b7d447: Waiting\n",
            "b23d47b63e15: Waiting\n",
            "614c3f8a359c: Waiting\n",
            "a9e218674e04: Waiting\n",
            "ae9158965e0c: Waiting\n",
            "2b3fcd0d7124: Waiting\n",
            "ccd31c9fc36f: Waiting\n",
            "5930c9e5703f: Waiting\n",
            "a47bd7f6fa0a: Waiting\n",
            "2dfe077478e7: Pushed\n",
            "4274592cc5a6: Pushed\n",
            "f87ce5b54ede: Pushed\n",
            "3b32d76aaeac: Pushed\n",
            "e504c5c14c58: Pushed\n",
            "90f0739e864f: Pushed\n",
            "915acdc59dc7: Pushed\n",
            "5e7be15cc7ac: Pushed\n",
            "ff925d82746c: Pushed\n",
            "fe58db7e29c9: Pushed\n",
            "73361934b445: Pushed\n",
            "1aa0fa3a5852: Pushed\n",
            "f1b6c74c4ba6: Pushed\n",
            "a1aa70b7d447: Pushed\n",
            "45f95fb7a389: Pushed\n",
            "a9e218674e04: Pushed\n",
            "96a152209e9e: Pushed\n",
            "5a61e03e9290: Pushed\n",
            "88a6021ce6f5: Pushed\n",
            "b580f8f66dce: Pushed\n",
            "8fb506f58648: Pushed\n",
            "b0f781117251: Pushed\n",
            "9db1bef051b5: Pushed\n",
            "de7dac334f31: Pushed\n",
            "a02c84464441: Pushed\n",
            "da6ea20315f6: Pushed\n",
            "b23d47b63e15: Pushed\n",
            "a47bd7f6fa0a: Pushed\n",
            "614c3f8a359c: Pushed\n",
            "\n",
            "ccd31c9fc36f: Pushed\n",
            "5277acf97db7: Pushed\n",
            "2b3fcd0d7124: Pushed\n",
            "add1badbdc49: Pushed\n",
            "a4aa8b6c47b3: Pushed\n",
            "14f6719c9a9f: Pushed\n",
            "603726fa9d3f: Pushed\n",
            "7ae4f18769b2: Pushed\n",
            "9202e74cdbdb: Pushed\n",
            "5709c385e246: Pushed\n",
            "ccf068a4d377: Pushed\n",
            "5428ff5c945d: Pushed\n",
            "461f1a8abdb4: Pushed\n",
            "0bfaec991053: Pushed\n",
            "d7d0183824f6: Pushed\n",
            "59f16337b409: Pushed\n",
            "c8a7f9a65aad: Pushed\n",
            "a33a567a8065: Pushed\n",
            "7fd727c3ea0d: Pushed\n",
            "0e38537de292: Pushed\n",
            "4f31588a31be: Pushed\n",
            "89c75506f8aa: Pushed\n",
            "9d76323a95cf: Pushed\n",
            "049699798ae6: Pushed\n",
            "2b9bb5b31e2a: Pushed\n",
            "3e70085060ca: Pushed\n",
            "6b481735d372: Pushed\n",
            "b7d470eb74cc: Pushed\n",
            "96c055a01732: Pushed\n",
            "ddc500d84994: Pushed\n",
            "c64c52ea2c16: Pushed\n",
            "5930c9e5703f: Pushed\n",
            "9b1bc2360477: Pushed\n",
            "b187ff70b2e4: Pushed\n",
            "c40a36ccac9e: Pushed\n",
            "ae9158965e0c: Pushed\n",
            "ed90e4663409: Pushed\n",
            "latest: digest: sha256:811074af0fc5ae10a9f10aef5d8df8501f35b5173254401f02ebf01c54d9380c size: 14130\n",
            "2020/11/05 03:20:41 Successfully pushed image: 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_c10f0953bc1e7da67ed17f33ab58f449:latest\n",
            "2020/11/05 03:20:41 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 332.235380)\n",
            "2020/11/05 03:20:41 Populating digests for step ID: acb_step_0...\n",
            "2020/11/05 03:20:45 Successfully populated digests for step ID: acb_step_0\n",
            "2020/11/05 03:20:45 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 473.077184)\n",
            "2020/11/05 03:20:45 The following dependencies were found:\n",
            "2020/11/05 03:20:45 \n",
            "- image:\n",
            "    registry: 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
            "    repository: azureml/azureml_c10f0953bc1e7da67ed17f33ab58f449\n",
            "    tag: latest\n",
            "    digest: sha256:811074af0fc5ae10a9f10aef5d8df8501f35b5173254401f02ebf01c54d9380c\n",
            "  runtime-dependency:\n",
            "    registry: mcr.microsoft.com\n",
            "    repository: azureml/openmpi3.1.2-nvidia-tritonserver20.07-py3\n",
            "    tag: latest\n",
            "    digest: sha256:ed9f36257833e30d0ed43df1e8f40d09ceb88c3d362b3f722a43fb1819263bbb\n",
            "  buildtime-dependency:\n",
            "  - registry: mcr.microsoft.com\n",
            "    repository: azureml/o16n-base/python-assets\n",
            "    tag: latest\n",
            "    digest: sha256:2b5b27abf2384c07ab46f7fdcad8193b8ca673c47fec49a12ae2c3c359168c9c\n",
            "  git: {}\n",
            "\n",
            "\n",
            "Run ID: ca3q was successful after 13m37s\n",
            "Package creation Succeeded\n",
            "Logging into Docker registry 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
            "Logging into Docker registry 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
            "Building Docker image from Dockerfile...\n",
            "Step 1/5 : FROM 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_c10f0953bc1e7da67ed17f33ab58f449\n",
            " ---> 52105735a82f\n",
            "Step 2/5 : COPY azureml-app /var/azureml-app\n",
            " ---> 2e0a7d2f9cbe\n",
            "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjY1NjA1NzVkLWZhMDYtNGU3ZC05NWZiLWY5NjJlNzRlZmQ3YSIsInJlc291cmNlR3JvdXBOYW1lIjoiYXp1cmVtbC1leGFtcGxlcyIsImFjY291bnROYW1lIjoiZGVmYXVsdCIsIndvcmtzcGFjZUlkIjoiMGUxNDk3NjQtMzcyMC00NjEwLWIwZjMtM2UzZjk3NDU0NGFjIn0sIm1vZGVscyI6e30sIm1vZGVsc0luZm8iOnt9fQ== | base64 --decode > /var/azureml-app/model_config_map.json\n",
            " ---> Running in 4a42b8fa03bb\n",
            " ---> 28f64ea355bf\n",
            "Step 4/5 : RUN mv '/var/azureml-app/tmpg5rpd2we.py' /var/azureml-app/main.py\n",
            " ---> Running in 65d483e56793\n",
            " ---> f6c21cf2ef1d\n",
            "Step 5/5 : CMD [\"bash\",\"-c\",\"source activate '/opt/miniconda'; export AZUREML_CONDA_ENVIRONMENT_PATH=${AZUREML_CONDA_ENVIRONMENT_PATH:=$CONDA_PREFIX}; exec 'runsvdir' '/var/runit'\"]\n",
            " ---> Running in aaadfc72f571\n",
            " ---> 4d13893c6dd7\n",
            "Successfully built 4d13893c6dd7\n",
            "Successfully tagged triton-densenet-onnx-local97789:latest\n",
            "Starting Docker container...\n",
            "Docker container running.\n",
            "Checking container health...\n",
            "Local webservice is running at http://localhost:6789\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.webservice import LocalWebservice\n",
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.model import InferenceConfig\n",
        "from random import randint\n",
        "\n",
        "service_name = \"triton-densenet-onnx-local\" + str(randint(10000, 99999))\n",
        "env = Environment(\"triton-example\")\n",
        "env.docker.base_image = None\n",
        "env.docker.base_dockerfile=prefix.joinpath(\"notebooks\", \"triton\", \"docker\", \"Dockerfile\")\n",
        "env.python.conda_dependencies=CondaDependencies()\n",
        "env.python.user_managed_dependencies=True\n",
        "env.python.interpreter_path='/opt/miniconda/bin/python'\n",
        "env.inferencing_stack_version='latest'\n",
        "\n",
        "\n",
        "inference_config = InferenceConfig(\n",
        "    # this entry script is where we dispatch a call to the Triton server\n",
        "    entry_script=\"dummy_score.py\",\n",
        "    source_directory=prefix.joinpath(\"code\", \"deployment\", \"triton\"),\n",
        "    environment=env,\n",
        ")\n",
        "\n",
        "config = LocalWebservice.deploy_configuration(port=6789)\n",
        "\n",
        "service = Model.deploy(\n",
        "    workspace=ws,\n",
        "    name=service_name,\n",
        "    models=[model],\n",
        "    inference_config=inference_config,\n",
        "    deployment_config=config,\n",
        "    overwrite=True,\n",
        ")\n",
        "\n",
        "service.wait_for_deployment(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Container has been successfully cleaned up.\n",
            "Starting Docker container...\n",
            "Docker container running.\n"
          ]
        }
      ],
      "source": [
        "service.reload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the webservice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unexpected shape for input 'data_0' for model 'densenet_onnx'. Expected [1,3,224,224], got [3,224,224]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "headers = {\"Content-Type\": \"application/octet-stream\"}\n",
        "\n",
        "data_file = prefix.joinpath(\"data\", \"raw\", \"images\", \"peacock.jpg\")\n",
        "test_sample = open(data_file, \"rb\").read()\n",
        "resp = requests.post(service.scoring_uri, data=test_sample, headers=headers)\n",
        "print(resp.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ilable.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:24,656057700+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:25,997636400+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:27,339959900+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:28,694041100+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:30,036082100+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:31,382078000+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:32,503044400+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:34,008144800+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:35,439927100+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:36,861198500+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:38,335556500+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:39,787654200+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:41,238012600+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:42,908345500+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:44,365921100+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:45,768319300+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n2020-11-05T04:26:47,733637100+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.07 (build 14649927)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\ntritonserver: /usr/lib/x86_64-linux-gnu/libcurl.so.4: version `CURL_OPENSSL_4' not found (required by /opt/tritonserver/bin/../lib/libtritonserver.so)\n\n"
          ]
        }
      ],
      "source": [
        "print(service.get_logs())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delete the webservice and the downloaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "service.delete()\n",
        "delete_triton_models(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next steps\n",
        "\n",
        "Try changing the deployment configuration to [deploy to Azure Kubernetes Service](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python) for higher availability and better scalability."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "Python 3.7.7 64-bit ('azureml': conda)",
      "display_name": "Python 3.7.7 64-bit ('azureml': conda)",
      "metadata": {
        "interpreter": {
          "hash": "53514593536e52de022f29ef618678eddccd581b6db5dc532e9838fb19203af5"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "name": "deploy-densenet-local",
    "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}