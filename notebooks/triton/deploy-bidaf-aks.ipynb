{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a question-answering model to the Triton Inference Server on NVIDIA Tesla V100s in Azure Kubernetes Service\n",
    "\n",
    "This notebook shows you how to deploy a Bi-Directional Attention Flow question-ansewring model to the high-performance [Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html) on Azure Kubernetes Service (AKS) graphical processing units (GPUs).\n",
    "\n",
    "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview steps\n",
    "\n",
    "Necessary only while this feature is in preview, will be unnecessary in a future release of the Azure Machine Learning Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "Model.Framework.MULTI = \"Multi\"\n",
    "Model._SUPPORTED_FRAMEWORKS_FOR_NO_CODE_DEPLOY.append(Model.Framework.MULTI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully downloaded model: densenet_onnx\n",
      "successfully downloaded model: bidaf-9\n",
      "successfully downloaded model: keiji_model\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# get the root of the repo\n",
    "prefix = Path(git.Repo(\".\", search_parent_directories=True).working_tree_dir)\n",
    "\n",
    "# Enables us to import helper functions as Python modules\n",
    "path_to_insert = prefix.joinpath(\"code\", \"deployment\", \"triton\").__str__()\n",
    "if path_to_insert not in sys.path:\n",
    "    sys.path.insert(1, path_to_insert)\n",
    "\n",
    "from model_utils import download_triton_models\n",
    "\n",
    "download_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model bidaf-9-example\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples'), name=bidaf-9-example, id=bidaf-9-example:33, version=33, tags={'area': 'Natural language processing', 'type': 'Question-answering'}, properties={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_path = prefix.joinpath(\"models\", \"triton\")\n",
    "\n",
    "model = Model.register(\n",
    "    model_path=model_path,\n",
    "    model_name=\"bidaf-9-example\",\n",
    "    tags={\"area\": \"Natural language processing\", \"type\": \"Question-answering\"},\n",
    "    description=\"Question answering from ONNX model zoo\",\n",
    "    workspace=ws,\n",
    "    model_framework=Model.Framework.MULTI\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy webservice\n",
    "\n",
    "Deploy to a pre-created [AksCompute](https://docs.microsoft.com/python/api/azureml-core/azureml.core.compute.aks.akscompute?view=azure-ml-py#provisioning-configuration-agent-count-none--vm-size-none--ssl-cname-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--location-none--vnet-resourcegroup-name-none--vnet-name-none--subnet-name-none--service-cidr-none--dns-service-ip-none--docker-bridge-cidr-none--cluster-purpose-none--load-balancer-type-none-) named `aks-gpu-deploy`. For other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running.\n",
      "Failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\n",
      "Operation ID: aa821d2d-2dbd-4d6c-938d-d5e1a42d73cb\n",
      "Current sub-operation type not known, more logs unavailable.\n",
      "Error:\n",
      "{\n",
      "  \"code\": \"BadRequest\",\n",
      "  \"statusCode\": 400,\n",
      "  \"message\": \"The request is invalid\",\n",
      "  \"details\": [\n",
      "    {\n",
      "      \"code\": \"DeploymentResourceInsufficient\",\n",
      "      \"message\": \"Unschedulable: CPU/Memory resource insufficient, 1 replicas unschedulable. Please add more node(s) with SKU more than 1.1 CPU Cores and 5.2GB Memory and 1 GPU Cores to host your replicas OR reduce your replica count and service resource requirements.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "ERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\n",
      "Operation ID: aa821d2d-2dbd-4d6c-938d-d5e1a42d73cb\n",
      "Current sub-operation type not known, more logs unavailable.\n",
      "Error:\n",
      "{\n",
      "  \"code\": \"BadRequest\",\n",
      "  \"statusCode\": 400,\n",
      "  \"message\": \"The request is invalid\",\n",
      "  \"details\": [\n",
      "    {\n",
      "      \"code\": \"DeploymentResourceInsufficient\",\n",
      "      \"message\": \"Unschedulable: CPU/Memory resource insufficient, 1 replicas unschedulable. Please add more node(s) with SKU more than 1.1 CPU Cores and 5.2GB Memory and 1 GPU Cores to host your replicas OR reduce your replica count and service resource requirements.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "WebserviceException",
     "evalue": "WebserviceException:\n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: aa821d2d-2dbd-4d6c-938d-d5e1a42d73cb\nCurrent sub-operation type not known, more logs unavailable.\nError:\n{\n  \"code\": \"BadRequest\",\n  \"statusCode\": 400,\n  \"message\": \"The request is invalid\",\n  \"details\": [\n    {\n      \"code\": \"DeploymentResourceInsufficient\",\n      \"message\": \"Unschedulable: CPU/Memory resource insufficient, 1 replicas unschedulable. Please add more node(s) with SKU more than 1.1 CPU Cores and 5.2GB Memory and 1 GPU Cores to host your replicas OR reduce your replica count and service resource requirements.\"\n    }\n  ]\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nOperation ID: aa821d2d-2dbd-4d6c-938d-d5e1a42d73cb\\nCurrent sub-operation type not known, more logs unavailable.\\nError:\\n{\\n  \\\"code\\\": \\\"BadRequest\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"The request is invalid\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"DeploymentResourceInsufficient\\\",\\n      \\\"message\\\": \\\"Unschedulable: CPU/Memory resource insufficient, 1 replicas unschedulable. Please add more node(s) with SKU more than 1.1 CPU Cores and 5.2GB Memory and 1 GPU Cores to host your replicas OR reduce your replica count and service resource requirements.\\\"\\n    }\\n  ]\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36mwait_for_deployment\u001b[0;34m(self, show_output)\u001b[0m\n\u001b[1;32m    751\u001b[0m                                           '{}'.format(self.state, self._operation_endpoint.split('/')[-1],\n\u001b[0;32m--> 752\u001b[0;31m                                                       logs_response, error_response), logger=module_logger)\n\u001b[0m\u001b[1;32m    753\u001b[0m             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\n",
      "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: aa821d2d-2dbd-4d6c-938d-d5e1a42d73cb\nCurrent sub-operation type not known, more logs unavailable.\nError:\n{\n  \"code\": \"BadRequest\",\n  \"statusCode\": 400,\n  \"message\": \"The request is invalid\",\n  \"details\": [\n    {\n      \"code\": \"DeploymentResourceInsufficient\",\n      \"message\": \"Unschedulable: CPU/Memory resource insufficient, 1 replicas unschedulable. Please add more node(s) with SKU more than 1.1 CPU Cores and 5.2GB Memory and 1 GPU Cores to host your replicas OR reduce your replica count and service resource requirements.\"\n    }\n  ]\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nOperation ID: aa821d2d-2dbd-4d6c-938d-d5e1a42d73cb\\nCurrent sub-operation type not known, more logs unavailable.\\nError:\\n{\\n  \\\"code\\\": \\\"BadRequest\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"The request is invalid\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"DeploymentResourceInsufficient\\\",\\n      \\\"message\\\": \\\"Unschedulable: CPU/Memory resource insufficient, 1 replicas unschedulable. Please add more node(s) with SKU more than 1.1 CPU Cores and 5.2GB Memory and 1 GPU Cores to host your replicas OR reduce your replica count and service resource requirements.\\\"\\n    }\\n  ]\\n}\"\n    }\n}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36mwait_for_deployment\u001b[0;34m(self, show_output)\u001b[0m\n\u001b[1;32m    759\u001b[0m                                           'Current state is {}'.format(self.state), logger=module_logger)\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mWebserviceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_operation_to_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: aa821d2d-2dbd-4d6c-938d-d5e1a42d73cb\nCurrent sub-operation type not known, more logs unavailable.\nError:\n{\n  \"code\": \"BadRequest\",\n  \"statusCode\": 400,\n  \"message\": \"The request is invalid\",\n  \"details\": [\n    {\n      \"code\": \"DeploymentResourceInsufficient\",\n      \"message\": \"Unschedulable: CPU/Memory resource insufficient, 1 replicas unschedulable. Please add more node(s) with SKU more than 1.1 CPU Cores and 5.2GB Memory and 1 GPU Cores to host your replicas OR reduce your replica count and service resource requirements.\"\n    }\n  ]\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nOperation ID: aa821d2d-2dbd-4d6c-938d-d5e1a42d73cb\\nCurrent sub-operation type not known, more logs unavailable.\\nError:\\n{\\n  \\\"code\\\": \\\"BadRequest\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"The request is invalid\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"DeploymentResourceInsufficient\\\",\\n      \\\"message\\\": \\\"Unschedulable: CPU/Memory resource insufficient, 1 replicas unschedulable. Please add more node(s) with SKU more than 1.1 CPU Cores and 5.2GB Memory and 1 GPU Cores to host your replicas OR reduce your replica count and service resource requirements.\\\"\\n    }\\n  ]\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from azureml.core.webservice import AksWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from random import randint\n",
    "\n",
    "service_name = \"triton-bidaf-9\" + str(randint(10000, 99999))\n",
    "\n",
    "config = AksWebservice.deploy_configuration(\n",
    "    compute_target_name=\"aks-gpu-deploy\",\n",
    "    gpu_cores=1,\n",
    "    cpu_cores=1,\n",
    "    memory_gb=4,\n",
    "    auth_enabled=False,\n",
    ")\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[model],\n",
    "    deployment_config=config,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in ws.webservices:\n",
    "    if service[:6]== 'triton':\n",
    "        service = AksWebservice(ws, service)\n",
    "        service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = AksWebservice(ws, 'triton-bidaf-962220')\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model: bidaf-9, version: 1,               input meta: [{'name': 'query_char', 'datatype': 'BYTES', 'shape': [-1, 1, 1, 16]}, {'name': 'query_word', 'datatype': 'BYTES', 'shape': [-1, 1]}, {'name': 'context_word', 'datatype': 'BYTES', 'shape': [-1, 1]}, {'name': 'context_char', 'datatype': 'BYTES', 'shape': [-1, 1, 1, 16]}], input config: [{'name': 'query_char', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1', '1', '16']}, {'name': 'query_word', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1']}, {'name': 'context_word', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1']}, {'name': 'context_char', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1', '1', '16']}],               output_meta: [{'name': 'end_pos', 'datatype': 'INT32', 'shape': [1]}, {'name': 'start_pos', 'datatype': 'INT32', 'shape': [1]}], output config: [{'name': 'end_pos', 'data_type': 'TYPE_INT32', 'dims': ['1']}, {'name': 'start_pos', 'data_type': 'TYPE_INT32', 'dims': ['1']}]\n",
      "Found model: densenet_onnx, version: 1,               input meta: [{'name': 'data_0', 'datatype': 'FP32', 'shape': [3, 224, 224]}], input config: [{'name': 'data_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NCHW', 'dims': ['3', '224', '224'], 'reshape': {'shape': ['1', '3', '224', '224']}}],               output_meta: [{'name': 'fc6_1', 'datatype': 'FP32', 'shape': [1000]}], output config: [{'name': 'fc6_1', 'data_type': 'TYPE_FP32', 'dims': ['1000'], 'label_filename': 'densenet_labels.txt', 'reshape': {'shape': ['1', '1000', '1', '1']}}]\n",
      "Found model: keiji_model, version: 1,               input meta: [{'name': 'msg_type_ids', 'datatype': 'INT64', 'shape': [-1, 64]}, {'name': 'msg_ids', 'datatype': 'INT64', 'shape': [-1, 64]}, {'name': 'msg_mask', 'datatype': 'INT64', 'shape': [-1, 64]}], input config: [{'name': 'msg_type_ids', 'data_type': 'TYPE_INT64', 'dims': ['-1', '64']}, {'name': 'msg_ids', 'data_type': 'TYPE_INT64', 'dims': ['-1', '64']}, {'name': 'msg_mask', 'data_type': 'TYPE_INT64', 'dims': ['-1', '64']}],               output_meta: [{'name': 'out_0', 'datatype': 'INT64', 'shape': [3]}], output config: [{'name': 'out_0', 'data_type': 'TYPE_INT64', 'dims': ['3']}]\n"
     ]
    }
   ],
   "source": [
    "from utils import triton_init, get_model_info\n",
    "\n",
    "\n",
    "triton_init(service.scoring_uri)\n",
    "\n",
    "get_model_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade nltk geventhttpclient python-rapidjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model: bidaf-9, version: 1,               input meta: [{'name': 'query_char', 'datatype': 'BYTES', 'shape': [-1, 1, 1, 16]}, {'name': 'query_word', 'datatype': 'BYTES', 'shape': [-1, 1]}, {'name': 'context_word', 'datatype': 'BYTES', 'shape': [-1, 1]}, {'name': 'context_char', 'datatype': 'BYTES', 'shape': [-1, 1, 1, 16]}], input config: [{'name': 'query_char', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1', '1', '16']}, {'name': 'query_word', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1']}, {'name': 'context_word', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1']}, {'name': 'context_char', 'data_type': 'TYPE_STRING', 'dims': ['-1', '1', '1', '16']}],               output_meta: [{'name': 'end_pos', 'datatype': 'INT32', 'shape': [1]}, {'name': 'start_pos', 'datatype': 'INT32', 'shape': [1]}], output config: [{'name': 'end_pos', 'data_type': 'TYPE_INT32', 'dims': ['1']}, {'name': 'start_pos', 'data_type': 'TYPE_INT32', 'dims': ['1']}]\n",
      "Found model: densenet_onnx, version: 1,               input meta: [{'name': 'data_0', 'datatype': 'FP32', 'shape': [3, 224, 224]}], input config: [{'name': 'data_0', 'data_type': 'TYPE_FP32', 'format': 'FORMAT_NCHW', 'dims': ['3', '224', '224'], 'reshape': {'shape': ['1', '3', '224', '224']}}],               output_meta: [{'name': 'fc6_1', 'datatype': 'FP32', 'shape': [1000]}], output config: [{'name': 'fc6_1', 'data_type': 'TYPE_FP32', 'dims': ['1000'], 'label_filename': 'densenet_labels.txt', 'reshape': {'shape': ['1', '1000', '1', '1']}}]\n",
      "Found model: keiji_model, version: 1,               input meta: [{'name': 'msg_type_ids', 'datatype': 'INT64', 'shape': [-1, 64]}, {'name': 'msg_ids', 'datatype': 'INT64', 'shape': [-1, 64]}, {'name': 'msg_mask', 'datatype': 'INT64', 'shape': [-1, 64]}], input config: [{'name': 'msg_type_ids', 'data_type': 'TYPE_INT64', 'dims': ['-1', '64']}, {'name': 'msg_ids', 'data_type': 'TYPE_INT64', 'dims': ['-1', '64']}, {'name': 'msg_mask', 'data_type': 'TYPE_INT64', 'dims': ['-1', '64']}],               output_meta: [{'name': 'out_0', 'datatype': 'INT64', 'shape': [3]}], output config: [{'name': 'out_0', 'data_type': 'TYPE_INT64', 'dims': ['3']}]\n",
      "None\n",
      "request is [\"A quick brown fox jumped over the lazy dog.\", \"Which animal was lower?\"] type is <class 'str'>\n",
      "start is 7, end is 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'lazy', b'dog']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Using a modified version of tritonhttpclient for Preview, PR is out for review\n",
    "# https://github.com/triton-inference-server/server/pull/2047\n",
    "\n",
    "from bidaf_utils import init, run\n",
    "\n",
    "init(service.scoring_uri)\n",
    "\n",
    "data = [\n",
    "    \"A quick brown fox jumped over the lazy dog.\",\n",
    "    \"Which animal was lower?\",\n",
    "]\n",
    "run(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the webservice and the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model_utils import delete_triton_models\n",
    "\n",
    "service.delete()\n",
    "delete_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Try reading [our documentation](https://aka.ms/triton-aml-docs) to use Triton with your own models or check out the other notebooks in this folder for ways to do pre- and post-processing on the server. "
   ]
  }
 ],
 "metadata": {
  "index": {
   "compute": "AKS - GPU",
   "frameworks": "onnx",
   "other": "triton no-code deploy",
   "scenario": "deployment"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "deploy-bidaf-aks",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
